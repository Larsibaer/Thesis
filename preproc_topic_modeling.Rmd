---
  title: "Topic Modeling"
author: "Lars Wenger"
date: "2024-03-26"
output: 
  html_document:
  toc: true
toc_float: true
---
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, results = "hide")
```

# Load Packages and Set Working Directory

```{r}
# Load necessary libraries
library(tidyverse)
library(quanteda)
library(stopwords)
library(topicmodels)
library(tidytext)
library(quanteda.textplots)
options(scipen=999)
```

# Load and Select Data

```{r}
# Clear the workspace
rm(list=ls())

# Set working directory
setwd("C:/Thesis")

# Read CSV file
cases_data <- read_csv("Data/TopicModel_VectorizedText_description.csv", locale = locale(encoding = "UTF-8"))
descriptions <- cases_data
```

# Prepare Text Data

```{r}
# Filter descriptions based on minimum text length
descriptions <- subset(descriptions, nchar(descriptions$description) > 100)

# Remove NAs in description
descriptions <- descriptions[!is.na(descriptions$description),]

# Remove unnecessary hyphens
descriptions$description <- gsub("-", " ", descriptions$description)

# Tokenize text and apply basic preprocessing
tokens <- tokens(descriptions$description,
                 remove_punct = TRUE,
                 remove_symbols = TRUE,
                 remove_numbers = TRUE,
                 remove_url = TRUE,
                 remove_separators = TRUE)

# Create n-grams
tokens <- tokens_ngrams(tokens, n = 1:2)

# Create Document-Feature Matrix
myDfm <- dfm(tokens)
```


# Analyze Text
```{r}
# Create LDA model (specify number of topics)
descriptions_lda <- LDA(myDfm, k = 7, control = list(seed = 123))
topics <- as.data.frame(terms(descriptions_lda, 50))

# Convert to tidy format for visualization
descriptions_lda_td <- tidy(descriptions_lda)

# Rename topic column values
descriptions_lda_td$topic <- factor(descriptions_lda_td$topic)
descriptions_lda_td$topic <- factor(descriptions_lda_td$topic, labels = c(
  "Network Server", "Performance and Response Issues", "VDI and Hosted Desktop",
  "Authentication and Accounts", "Office Applications", "Printing and Drive", 
  "Support Infrastructure"))

# Save the tidy LDA results
write_csv(descriptions_lda_td, "Data/topicModel_descriptions_lda_td.csv")

# Extract top terms per topic
top_terms <- descriptions_lda_td %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

# Visualize top terms and their loadings
top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()

# Link Results to Metadata

# Get topic distributions for each document
tmResult <- posterior(descriptions_lda)
theta <- tmResult$topics
lda_results <- cbind(descriptions, theta)

# Add missing rows from cases_data with topic values set to 0
lda_results <- lda_results[-2] %>%
  full_join(cases_data, by = "number")

# Fill NA values with 0
lda_results[is.na(lda_results)] <- 0

# Remove last column
lda_results <- lda_results[-ncol(lda_results)]

# Rename columns
colnames(lda_results) <- c("number", "topic_network_server", "topic_performance_responseIssues", 
                           "topic_vdi_hostedDesktop", "topic_authentication_accounts", 
                           "topic_officeApplications", "topic_printing_drive", "topic_support_infrastructure")

# Save the final dataframe
write_csv(lda_results, "Data/topicModel_description.csv")
```

# Choose the Best Number of Topics

```{r}
# Load ldatuning library
library(ldatuning)

# Calculate different metrics to estimate the best number of topics for LDA model
result <- FindTopicsNumber(
  myDfm,
  topics = seq(from = 2, to = 10, by = 1),
  metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
  method = "Gibbs",
  control = list(seed = 77),
  mc.cores = 2L,
  verbose = TRUE)

# Plot results
FindTopicsNumber_plot(result)
```

# Combine All Text Information

```{r}
# Read additional text data
close_notes <- read_csv("Data/TopicModel_VectorizedText_close_notes.csv", locale = locale(encoding = "UTF-8"))
cause <- read_csv("Data/TopicModel_VectorizedText_cause.csv", locale = locale(encoding = "UTF-8"))

# Combine all text information
all_text <- merge(close_notes, cause, by = "number", all = TRUE)
all_text <- merge(all_text, descriptions, by = "number", all = TRUE)

# Concatenate all text fields into one
all_text$all_text <- paste(all_text$close_notes, all_text$cause, all_text$description, sep = " ")

# Replace "NA" text with empty strings and trim white spaces
all_text$all_text <- gsub("NA", "", all_text$all_text)
all_text$all_text <- trimws(all_text$all_text)

# Fill empty text with NA
all_text$all_text[all_text$all_text == ""] <- NA

# Remove individual text columns
all_text <- all_text %>%
  select(-close_notes, -cause, -description)

# Process Combined Text Data

# Remove NAs in combined text data
all_text <- all_text[!is.na(all_text$all_text),]

# Tokenize combined text data
new_tokens <- tokens(all_text$all_text,
                     remove_punct = TRUE,
                     remove_symbols = TRUE,
                     remove_numbers = TRUE,
                     remove_url = TRUE,
                     remove_separators = TRUE)

# Create n-grams for combined text data
new_tokens <- tokens_ngrams(new_tokens, n = 1:2)

# Create Document-Feature Matrix for new data
new_myDfm <- dfm(new_tokens, verbose = FALSE)

# Apply the trained LDA model to the new data
new_descriptions_lda <- LDA(new_myDfm, model = descriptions_lda)

# Get topic distributions for the new data
tmResult <- posterior(new_descriptions_lda)
theta <- tmResult$topics
lda_results2 <- cbind(all_text, theta)

# Clean up temporary variables
rm(theta, new_descriptions_lda_td, tmResult, top_terms, tokens)

# Add missing rows from cases_data with topic values set to 0
lda_results2 <- lda_results2[-2]
lda_results2[is.na(lda_results2)] <- 0

# Rename columns
colnames(lda_results2) <- c("number", "topic_network_server", "topic_performance_responseIssues", 
                            "topic_vdi_hostedDesktop", "topic_authentication_accounts", 
                            "topic_officeApplications", "topic_printing_drive", "topic_support_infrastructure")

# Save the final dataframe
write_csv(lda_results2, "Data/topicModel_allText.csv")
```
