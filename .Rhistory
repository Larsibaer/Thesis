final_data$cluster <- as.factor(final_data$cluster) # Ensure the cluster variable is a factor
# Summary statistics for each cluster
summary_stats <- final_data %>%
group_by(cluster) %>%
summarise_all(funs(mean = mean(., na.rm = TRUE), sd = sd(., na.rm = TRUE)))
# Boxplots for a variable across clusters
ggplot(final_data, aes(x = cluster, y = final_data$openedToClosed)) +
geom_boxplot() +
theme_minimal() +
labs(title = "Distribution of Opened to Closed Across Clusters", y = "Opened to Closed (Hours)", x = "Cluster")
dbscan_data1 <- dbscan(encoded_df, eps = 2, MinPts = 100)
cluster_data <- data.frame(number = data$number, cluster = dbscan_data1$cluster)
# Merge the cluster information back to the original data
final_data <- merge(cases_data, cluster_data, by = "number", all.x = TRUE)
# Count the number of stocks in each cluster
table(final_data$cluster)
#Median of openedToClosed as number
summary(final_data[final_data$cluster == c(1,2,3,4,5,6,8),])
#summary me the distibution of the cluster 7
summary(final_data[final_data$cluster == 7,])
# Assuming 'data' is your dataset and 'cluster' is the column indicating cluster membership
final_data$cluster <- as.factor(final_data$cluster) # Ensure the cluster variable is a factor
# Summary statistics for each cluster
summary_stats <- final_data %>%
group_by(cluster) %>%
summarise_all(funs(mean = mean(., na.rm = TRUE), sd = sd(., na.rm = TRUE)))
# Boxplots for a variable across clusters
ggplot(final_data, aes(x = cluster, y = final_data$openedToClosed)) +
geom_boxplot() +
theme_minimal() +
labs(title = "Distribution of Opened to Closed Across Clusters", y = "Opened to Closed (Hours)", x = "Cluster")
View(encoded_df)
dbscan_data1 <- dbscan(encoded_df, eps = 1.5, MinPts = 100)
cluster_data <- data.frame(number = data$number, cluster = dbscan_data1$cluster)
# Merge the cluster information back to the original data
final_data <- merge(cases_data, cluster_data, by = "number", all.x = TRUE)
# Count the number of stocks in each cluster
table(final_data$cluster)
#Median of openedToClosed as number
summary(final_data[final_data$cluster == c(1,2,3,4,5,6,8),])
#summary me the distibution of the cluster 7
summary(final_data[final_data$cluster == 7,])
# Assuming 'data' is your dataset and 'cluster' is the column indicating cluster membership
final_data$cluster <- as.factor(final_data$cluster) # Ensure the cluster variable is a factor
# Summary statistics for each cluster
summary_stats <- final_data %>%
group_by(cluster) %>%
summarise_all(funs(mean = mean(., na.rm = TRUE), sd = sd(., na.rm = TRUE)))
# Boxplots for a variable across clusters
ggplot(final_data, aes(x = cluster, y = final_data$openedToClosed)) +
geom_boxplot() +
theme_minimal() +
labs(title = "Distribution of Opened to Closed Across Clusters", y = "Opened to Closed (Hours)", x = "Cluster")
#Median of openedToClosed as number
summary(final_data[final_data$cluster == c(1:14,16:19),])
#summary me the distibution of the cluster 7
summary(final_data[final_data$cluster == 15,])
#Median of openedToClosed as number
summary(final_data[final_data$cluster == c(1:14,16:19),])
#summary me the distibution of the cluster 7
summary(final_data[final_data$cluster == 15,])
# Count the number of stocks in each cluster
table(final_data$cluster)
#Median of openedToClosed as number
summary(final_data[final_data$cluster == c(1:19),])
knitr::opts_chunk$set(echo = TRUE)
libraries = c( "fpp3", "ggplot2", "dplyr", "tidyr", "readxl", "forecast", "zoo", "tsibble", "GGally", "lubridate", "tidyverse", "stringi")
lapply(libraries, function(x) if (!(x %in% installed.packages())) {
install.packages(x)
})
lapply(libraries, library, quietly = TRUE, character.only = TRUE)
rm(list=ls())
setwd("C:/Thesis")
data <- read_csv("Data/data.csv", locale = locale(encoding = "UTF-8"))
cluster_data <- read_csv("Data/data_withclusters.csv", locale = locale(encoding = "UTF-8"))
topic_model_data <- read_csv("Data/topicModel_description.csv", locale = locale(encoding = "UTF-8"))
vect_desc_data <- read_csv("Data/VectorizedText_description.csv", locale = locale(encoding = "UTF-8"))
vect_cause_data <- read_csv("Data/VectorizedText_cause.csv", locale = locale(encoding = "UTF-8"))
vect_clause_notes_data <- read_csv("Data/VectorizedText_close_notes.csv", locale = locale(encoding = "UTF-8"))
proportion_data <- read_csv("Data/data_proportion.csv", locale = locale(encoding = "UTF-8"))
#Merge all data by number
data_all <- data %>%
inner_join(cluster_data, by = c("number" = "number")) %>%
inner_join(topic_model_data, by = c("number" = "number")) %>%
inner_join(vect_desc_data, by = c("number" = "number")) %>%
inner_join(vect_cause_data, by = c("number" = "number")) %>%
inner_join(vect_clause_notes_data, by = c("number" = "number")) %>%
inner_join(proportion_data, by = c("number" = "number"))
View(data)
write.csv(data, "Data/data_all.csv", row.names = FALSE)
knitr::opts_chunk$set(echo = TRUE)
libraries = c( "fpp3", "ggplot2", "dplyr", "tidyr", "readxl", "forecast", "zoo", "tsibble", "GGally", "lubridate", "tidyverse", "stringi")
lapply(libraries, function(x) if (!(x %in% installed.packages())) {
install.packages(x)
})
lapply(libraries, library, quietly = TRUE, character.only = TRUE)
rm(list=ls())
setwd("C:/Thesis")
data <- read_csv("Data/data.csv", locale = locale(encoding = "UTF-8"))
cluster_data <- read_csv("Data/data_withclusters.csv", locale = locale(encoding = "UTF-8"))
topic_model_data <- read_csv("Data/topicModel_description.csv", locale = locale(encoding = "UTF-8"))
vect_desc_data <- read_csv("Data/VectorizedText_description.csv", locale = locale(encoding = "UTF-8"))
vect_cause_data <- read_csv("Data/VectorizedText_cause.csv", locale = locale(encoding = "UTF-8"))
vect_clause_notes_data <- read_csv("Data/VectorizedText_close_notes.csv", locale = locale(encoding = "UTF-8"))
proportion_data <- read_csv("Data/data_proportion.csv", locale = locale(encoding = "UTF-8"))
#Merge all data by number
data_all <- data %>%
inner_join(cluster_data, by = c("number" = "number")) %>%
inner_join(topic_model_data, by = c("number" = "number")) %>%
inner_join(vect_desc_data, by = c("number" = "number")) %>%
inner_join(vect_cause_data, by = c("number" = "number")) %>%
inner_join(vect_clause_notes_data, by = c("number" = "number")) %>%
inner_join(proportion_data, by = c("number" = "number"))
#Save file without index
write.csv(data_all, "Data/data_all.csv", row.names = FALSE)
cases_data2 <- read_csv("Data/TopicModel_VectorizedText_close_notes.csv", locale = locale(encoding = "UTF-8"))
cases_data3 <- read_csv("Data/TopicModel_VectorizedText_cause.csv", locale = locale(encoding = "UTF-8"))
# Set working directory
setwd("C:/Thesis")
cases_data2 <- read_csv("Data/TopicModel_VectorizedText_close_notes.csv", locale = locale(encoding = "UTF-8"))
cases_data3 <- read_csv("Data/TopicModel_VectorizedText_cause.csv", locale = locale(encoding = "UTF-8"))
View(cases_data2)
View(cases_data3)
descriptions <- cases_data
# OPTIONAL: Specify minimum text length (number of characters)
descriptions <- subset(descriptions, descriptions$description > 100)
# remove NAs in description
descriptions <- descriptions[!is.na(descriptions$description),]
#read json file
cases_data <- read_csv("Data/TopicModel_VectorizedText_description.csv", locale = locale(encoding = "UTF-8"))
descriptions <- cases_data
# OPTIONAL: Specify minimum text length (number of characters)
descriptions <- subset(descriptions, descriptions$description > 100)
# remove NAs in description
descriptions <- descriptions[!is.na(descriptions$description),]
### - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
### STEP 3: PREPARE TEXT DATA (TEXT PRE-PROCESSING)
### - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
# Remove all "-" where they are not needed
descriptions$description <- gsub("-", " ", descriptions$description)
# Transform words into tokens, select basic text preprocessing steps
tokens <- tokens(descriptions$description,
remove_punct = TRUE,
remove_symbols = TRUE,
remove_numbers = TRUE,
remove_url = TRUE,
remove_separators = TRUE)
### - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
### STEP 1: LOAD PACKAGES AND SET WORKING DIRECTORY
### - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
library(tidyverse)
library(quanteda)
library(stopwords)
library(topicmodels)
library(tidytext)
library(quanteda.textplots)
options(scipen=999)
rm(list=ls())
# Set working directory
setwd("C:/Thesis")
#read json file
cases_data <- read_csv("Data/TopicModel_VectorizedText_description.csv", locale = locale(encoding = "UTF-8"))
descriptions <- cases_data
# OPTIONAL: Specify minimum text length (number of characters)
descriptions <- subset(descriptions, descriptions$description > 100)
# remove NAs in description
descriptions <- descriptions[!is.na(descriptions$description),]
### - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
### STEP 3: PREPARE TEXT DATA (TEXT PRE-PROCESSING)
### - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
# Remove all "-" where they are not needed
descriptions$description <- gsub("-", " ", descriptions$description)
# Transform words into tokens, select basic text preprocessing steps
tokens <- tokens(descriptions$description,
remove_punct = TRUE,
remove_symbols = TRUE,
remove_numbers = TRUE,
remove_url = TRUE,
remove_separators = TRUE)
# Create n-grams of any length
tokens <- tokens_ngrams(tokens, n = 1:2)
# Create Document-feature-matrix
myDfm <-dfm(tokens)
View(descriptions)
# merge the two dataframes
cases_data4 <- rbind(cases_data2, cases_data3)
cases_data2 <- read_csv("Data/TopicModel_VectorizedText_close_notes.csv", locale = locale(encoding = "UTF-8"))
cases_data3 <- read_csv("Data/TopicModel_VectorizedText_cause.csv", locale = locale(encoding = "UTF-8"))
# merge the two dataframes
cases_data4 <- rbind(cases_data2, cases_data3)
View(cases_data2)
# merge the the second columns of cases_data2 and cases_data3 and description
cases_data2 <- cases_data2[-1]
cases_data3 <- cases_data3[-1]
cases_data4 <- cbind(cases_data2, cases_data3)
View(cases_data4)
# merge the the second columns of cases_data2 and cases_data3 and description
# Merging df1 and df2
merged_df <- merge(cases_data, cases_data2, by = "number", all = TRUE)
# merge the the second columns of cases_data2 and cases_data3 and description
# Merging df1 and df2
merged_df <- merge(cases_data, cases_data2, by = "number", all = TRUE)
cases_data2 <- read_csv("Data/TopicModel_VectorizedText_close_notes.csv", locale = locale(encoding = "UTF-8"))
cases_data3 <- read_csv("Data/TopicModel_VectorizedText_cause.csv", locale = locale(encoding = "UTF-8"))
# merge the the second columns of cases_data2 and cases_data3 and description
# Merging df1 and df2
merged_df <- merge(cases_data, cases_data2, by = "number", all = TRUE)
View(merged_df)
# Now merge the result with df3
final_df <- merge(merged_df, cases_data3, by = "number", all = TRUE)
View(final_df)
# Sum the columns 2,3 and 4 to one column
final_df$text <- paste(final_df$close_notes, final_df$cause, final_df$description, sep = " ")
View(final_df)
#Remove all "NA" in text
final_df$text <- gsub("NA", "", final_df$text)
View(final_df)
# Load the 'quanteda' library for text processing if not already loaded
library(quanteda)
# Preprocess text
new_tokens <- tokens(final_df$text,
remove_punct = TRUE,
remove_symbols = TRUE,
remove_numbers = TRUE,
remove_url = TRUE,
remove_separators = TRUE)
# Generate n-grams
new_tokens <- tokens_ngrams(new_tokens, n = 1:2)
# Create a document-feature matrix using the same dictionary as the original model
new_dfm <- dfm(new_tokens, dictionary = dictionary(myDfm))
# Set working directory
setwd("C:/Thesis")
rm(list=ls())
# Set working directory
setwd("C:/Thesis")
#read json file
cases_data <- read_csv("Data/TopicModel_VectorizedText_description.csv", locale = locale(encoding = "UTF-8"))
descriptions <- cases_data
# OPTIONAL: Specify minimum text length (number of characters)
descriptions <- subset(descriptions, descriptions$description > 100)
# remove NAs in description
descriptions <- descriptions[!is.na(descriptions$description),]
### - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
### STEP 3: PREPARE TEXT DATA (TEXT PRE-PROCESSING)
### - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
# Remove all "-" where they are not needed
descriptions$description <- gsub("-", " ", descriptions$description)
# Transform words into tokens, select basic text preprocessing steps
tokens <- tokens(descriptions$description,
remove_punct = TRUE,
remove_symbols = TRUE,
remove_numbers = TRUE,
remove_url = TRUE,
remove_separators = TRUE)
# Create n-grams of any length
tokens <- tokens_ngrams(tokens, n = 1:2)
# Create Document-feature-matrix
myDfm <-dfm(tokens)
### - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
### STEP 4: ANALYZE TEXT
### - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
# Create LDA model (specify number of topics)
descriptions_lda <- LDA(myDfm, k = 7, control = list(seed = 123))
topics <- as.data.frame(terms(descriptions_lda, 50))
# Convert into tidy-format to visualize results
descriptions_lda_td <- tidy(descriptions_lda)
# Extract top-terms per topic
top_terms <- descriptions_lda_td %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
# Visualize top-terms and their loadings (can you assign topic labels based on this information?)
top_terms %>%
mutate(term = reorder(term, beta)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()
# Link results to metadata
tmResult <- posterior(descriptions_lda)
theta <- tmResult$topics
lda_results <- cbind(descriptions, theta)
lda_results <- lda_results[-2] %>%
full_join(cases_data, by = "number")
# fill NA with 0
lda_results[is.na(lda_results)] <- 0
# remove last column
lda_results <- lda_results[-ncol(lda_results)]
# Rename columns
colnames(lda_results) <- c("number", "topic_network_server", "topic_performance_responseIssues", "topic_vdi_hostedDesktop", "topic_authentication_accounts", "topic_officeApplications", "topic_printing_drive", "topic_support_infrastructure")
library (ldatuning)
# Calculate selected metrics
result <- FindTopicsNumber(
myDfm,
topics = seq(from = 2, to = 10, by = 1),
metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
method = "Gibbs",
control = list(seed = 77),
mc.cores = 2L,
verbose = TRUE)
# plot results
FindTopicsNumber_plot(result)
cases_data2 <- read_csv("Data/TopicModel_VectorizedText_close_notes.csv", locale = locale(encoding = "UTF-8"))
cases_data3 <- read_csv("Data/TopicModel_VectorizedText_cause.csv", locale = locale(encoding = "UTF-8"))
# merge the the second columns of cases_data2 and cases_data3 and description
# Merging df1 and df2
merged_df <- merge(cases_data, cases_data2, by = "number", all = TRUE)
# Now merge the result with df3
final_df <- merge(merged_df, cases_data3, by = "number", all = TRUE)
# Sum the columns 2,3 and 4 to one column
final_df$text <- paste(final_df$close_notes, final_df$cause, final_df$description, sep = " ")
#Remove all "NA" in text
final_df$text <- gsub("NA", "", final_df$text)
# Load the 'quanteda' library for text processing if not already loaded
library(quanteda)
# Preprocess text
new_tokens <- tokens(final_df$text,
remove_punct = TRUE,
remove_symbols = TRUE,
remove_numbers = TRUE,
remove_url = TRUE,
remove_separators = TRUE)
# Generate n-grams
new_tokens <- tokens_ngrams(new_tokens, n = 1:2)
# Create a document-feature matrix using the same dictionary as the original model
new_dfm <- dfm(new_tokens, dictionary = dictionary(myDfm))
# Get the list of features from the original DFM
original_features <- featnames(myDfm)
# Create a new document-feature matrix, filtered to only use the original features
new_dfm <- dfm(new_tokens)
new_dfm <- dfm_select(new_dfm, pattern = original_features, selection = "keep")
# Predict topic distribution using the LDA model
new_topics <- predict(descriptions_lda, newdata = new_dfm)
# Extract the highest probability topic per document or use the whole distribution
final_df$predicted_topic <- topics(new_topics, 1)  # gets the most probable topic per document
# Predict topic distribution using the LDA model
new_topics <- predict(descriptions_lda, newdata = new_dfm)
View(descriptions)
cases_data2 <- read_csv("Data/TopicModel_VectorizedText_close_notes.csv", locale = locale(encoding = "UTF-8"))
cases_data3 <- read_csv("Data/TopicModel_VectorizedText_cause.csv", locale = locale(encoding = "UTF-8"))
# merge the the second columns of cases_data2 and cases_data3 and description
# Merging df1 and df2
merged_df <- merge(cases_data, cases_data2, by = "number", all = TRUE)
# Now merge the result with df3
final_df <- merge(merged_df, cases_data3, by = "number", all = TRUE)
# Sum the columns 2,3 and 4 to one column
final_df$text <- paste(final_df$close_notes, final_df$cause, final_df$description, sep = " ")
#Remove all "NA" in text
final_df$text <- gsub("NA", "", final_df$text)
# OPTIONAL: Specify minimum text length (number of characters)
final_df <- subset(final_df, final_df$text > 100)
# remove NAs in description
final_df <- final_df[!is.na(final_df$text),]
### - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
### STEP 3: PREPARE TEXT DATA (TEXT PRE-PROCESSING)
### - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
# Remove all "-" where they are not needed
final_df$text <- gsub("-", " ", final_df$text)
# Transform words into tokens, select basic text preprocessing steps
tokens <- tokens(final_df$text,
remove_punct = TRUE,
remove_symbols = TRUE,
remove_numbers = TRUE,
remove_url = TRUE,
remove_separators = TRUE)
# Create n-grams of any length
tokens <- tokens_ngrams(tokens, n = 1:2)
# Create Document-feature-matrix
myDfm <-dfm(tokens)
### - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
### STEP 4: ANALYZE TEXT
### - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
# Create LDA model (specify number of topics)
texts_lda <- LDA(myDfm, k = 7, control = list(seed = 123))
topics <- as.data.frame(terms(texts_lda, 50))
# Convert into tidy-format to visualize results
texts_lda_td <- tidy(texts_lda)
# Extract top-terms per topic
top_terms <- texts_lda_td %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
# Visualize top-terms and their loadings (can you assign topic labels based on this information?)
top_terms %>%
mutate(term = reorder(term, beta)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()
# Link results to metadata
tmResult <- posterior(texts_lda)
theta <- tmResult$topics
lda_results <- cbind(texts, theta)
rm (theta, texts_lda,texts_lda_td,tmResult,top_terms,tokens)
lda_results <- lda_results[-2] %>%
full_join(cases_data, by = "number")
# fill NA with 0
lda_results[is.na(lda_results)] <- 0
# remove last column
lda_results <- lda_results[-ncol(lda_results)]
# Calculate selected metrics
# Calculate selected metrics
result <- FindTopicsNumber(
myDfm,
topics = seq(from = 2, to = 10, by = 1),
metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
method = "Gibbs",
control = list(seed = 77),
mc.cores = 2L,
verbose = TRUE)
knitr::opts_chunk$set(echo = TRUE)
libraries = c( "fpp3", "ggplot2", "dplyr", "tidyr", "readxl", "forecast", "zoo", "tsibble", "GGally", "lubridate", "tidyverse", "stringi")
lapply(libraries, function(x) if (!(x %in% installed.packages())) {
install.packages(x)
})
lapply(libraries, library, quietly = TRUE, character.only = TRUE)
rm(list=ls())
setwd("C:/Thesis")
data <- read_csv("Data/data.csv", locale = locale(encoding = "UTF-8"))
cluster_data <- read_csv("Data/data_withclusters.csv", locale = locale(encoding = "UTF-8"))
topic_model_data <- read_csv("Data/topicModel_description.csv", locale = locale(encoding = "UTF-8"))
vect_desc_data <- read_csv("Data/VectorizedText_description.csv", locale = locale(encoding = "UTF-8"))
vect_cause_data <- read_csv("Data/VectorizedText_cause.csv", locale = locale(encoding = "UTF-8"))
vect_clause_notes_data <- read_csv("Data/VectorizedText_close_notes.csv", locale = locale(encoding = "UTF-8"))
proportion_data <- read_csv("Data/data_proportion.csv", locale = locale(encoding = "UTF-8"))
#Merge all data by number
data_all <- data %>%
inner_join(cluster_data, by = c("number" = "number")) %>%
inner_join(topic_model_data, by = c("number" = "number")) %>%
inner_join(vect_desc_data, by = c("number" = "number")) %>%
inner_join(vect_cause_data, by = c("number" = "number")) %>%
inner_join(vect_clause_notes_data, by = c("number" = "number")) %>%
inner_join(proportion_data, by = c("number" = "number"))
# Do a copy of data_all and replace "Unico" with FuBar
data_gpt <- data_all
data_gpt$created_by_group[data_gpt$created_by_group == "Unico"] <- "FuBar"
#Rplace the column account with a Substring of the last 3 letters
data_gpt$account <-  paste0(substr(data_gpt$account, 1, 2), substr(data_gpt$account, nchar(data_gpt$account)-3, nchar(data_gpt$account)))
#remove columns 17-19
data_gpt <- data_gpt[,-c(17:19)]
state_index <- grep("prop_state", colnames(data_all))
group_index <- grep("prop_group", colnames(data_all))
topic_index <- grep("topic_", colnames(data_all))
#Data Selection
data_gpt <- data_gpt %>% select(cluster, created_by_group, account, openedToClosed, time_worked, topic_index, state_index, group_index, case_cause, resolution_code, impact, priority, urgency)
#Save file without index
write.csv(data_gpt, "Data/data_gpt.csv", row.names = FALSE)
data_all$cluster <- as.factor(data_all$cluster)
# Plot a Boxplot of each Cluster for openedToClosed and time_worked
ggplot(data_all, aes(x = cluster, y = data_all$openedToClosed)) +
geom_boxplot() +
theme_minimal() +
labs(title = "Distribution of Opened to Closed Across Clusters", y = "Opened to Closed (Hours)", x = "Cluster")
# Plot a ggplot bar Plot of Business Services for data_all cluster == 3, fill with data_prop_state. Show text in x axis in a 45 degree angle
# Histogram comparison
ggplot(data_all, aes(x = openedToClosed, fill = as.factor(cluster == c(7)))) +
geom_histogram(alpha = 0.5, position = "identity", binwidth = 20) +
scale_fill_manual(values = c("red", "blue"), labels = c("Other Clusters", "Cluster 7")) +
labs(x = "Duration (openedToClosed)", y = "Frequency", title = "Comparison of Ticket Duration: Cluster 7 vs Other Clusters") +
theme_minimal()
# Select the columns that contains "prop_state" in the name
state_index <- grep("prop_state", colnames(data_all))
# Make new datafram with columns 1-6 and 20-25
data_prop_state <- select(data_all, number, cluster, c(state_index))
# Remove prop_state_ of column names
colnames(data_prop_state) <- gsub("prop_state_", "", colnames(data_prop_state))
# grouping the data by the cluster number, calculating the average duration for tickets within each cluster, and then visualizing these averages to identify any notable differences among the clusters.
data_prop_state %>%
group_by(cluster) %>%
summarise_all(mean) %>%
pivot_longer(cols = c(3:length(data_prop_state)), names_to = "variable", values_to = "value") %>%
ggplot(aes(x = cluster, y = value, fill = variable)) +
geom_bar(stat = "identity", position = "dodge") +
labs(title = "Average Duration of Tickets by Cluster",
x = "Cluster",
y = "Average Duration",
fill = "Variable") +
theme_minimal()
# Select the columns that contains "prop_state" in the name
user_index <- grep("prop_user", colnames(data_all))
data_prop_user <- select(data_all, cluster, openedToClosed, c(user_index))
data_prop_user <- data_prop_user %>% filter(openedToClosed > 20)
# Remove openedToClosed column
data_prop_user <- data_prop_user[-2]
# Replace NA with 0
data_prop_user[is.na(data_prop_user)] <- 0
# Remove prop_user_ of column names
colnames(data_prop_user) <- gsub("prop_user_", "", colnames(data_prop_user))
#
# Add row with the mean of the columns
colSum <- rbind(data_prop_user, colMeans(data_prop_user[-1]))
# Get all column indexes, where last row is > 2
index <- which(colSum[nrow(colSum),] > 2.5)
# usering the data by the cluster number, calculating the average duration for tickets within each cluster, and then visualizing these averages to identify any notable differences among the clusters.
data_prop_user %>%
group_by(cluster) %>%
select(cluster, index) %>%
summarise_all(mean) %>%
pivot_longer(cols = c(2:6), names_to = "variable", values_to = "value") %>%
ggplot(aes(x = cluster, y = value, fill = variable)) +
geom_bar(stat = "identity", position = "dodge") +
labs(title = "Average Duration of Tickets by Cluster",
x = "Cluster",
y = "Average Duration",
fill = "Variable") +
theme_minimal()
# Select the columns that contains "prop_state" in the name
group_index <- grep("prop_group", colnames(data_all))
data_prop_group <- select(data_all, cluster, openedToClosed, c(group_index))
data_prop_group <- data_prop_group %>% filter(openedToClosed > 20 & prop_group_Service.Desk.1st.Level < 95)
