html <- read_html(stock_news$ArticleURL)
hhh <- stock_news$ArticleURL[2]
html <- read_html(stock_news$ArticleURL)
hhh <- stock_news$ArticleURL[3]
html <- read_html(stock_news$ArticleURL)
hhh <- stock_news$ArticleURL[4]
html <- read_html(stock_news$ArticleURL)
setwd("C:/Users/Student/polygon")
data_raw <- read_csv("stock_news2.csv")
stock_news <- data_raw %>% drop_na(ArticleURL)
hhh <- stock_news$ArticleURL[5]
html <- read_html(stock_news$ArticleURL)
hhh <- stock_news$ArticleURL[6]
html <- read_html(stock_news$ArticleURL)
hhh <- stock_news$ArticleURL[7]
html <- read_html(stock_news$ArticleURL)
hhh <- stock_news$ArticleURL[8]
html <- read_html(stock_news$ArticleURL)
hhh <- stock_news$ArticleURL[9]
html <- read_html(stock_news$ArticleURL)
typeof(hhh)
is.string(hhh)
typeof(toSting(hhh))
paste(hhh)
print(hhh)
hhh <- stock_news$ArticleURL[4]
typeof(hhh)
print(hhh)
paste(hhh)
typeof(hhh)
print(hhh[0])
print(hhh[1])
hhh <- stock_news$ArticleURL[4]
html <- read_html(hhh)
text_1 <- html %>% html_elements(xpath = '//*[@id="js-article__body"]/p') %>% html_text() %>% toString()
text_2 <- html %>% html_elements(xpath = '//*[@class="paywall"]/p') %>% html_text() %>% toString()
# Preporcessing Text
text <- paste(preprocess_text(text_1), preprocess_text(text_2))
ggg <-   if (text == ""){
return(description)
} else{
return(text)
}
text
ggg <-   if (text == ""){
description
} else{
text
}
# Wende die prepare_text-Funktion auf jede Zeile im DataFrame an
stock_news$preprocessed_text <- mapply(prepare_text, stock_news$Publisher, stock_news$ArticleURL, stock_news$Description)
prepare_text <- function(publisher, articleURL, description) {
if (is.na(description)){
return("")
}
else{
if (publisher == "Zacks Investment Research") {
return(preprocess_text(scrap_text_zack(articleURL, description)))
} else if (publisher == "MarketWatch") {
return(preprocess_text(scrap_text_marketwatch(articleURL, description)))
} else {
return(preprocess_text(description))
}
}
}
# Wende die prepare_text-Funktion auf jede Zeile im DataFrame an
stock_news$preprocessed_text <- mapply(prepare_text, stock_news$Publisher, stock_news$ArticleURL, stock_news$Description)
View(stock_news)
scrap_text_marketwatch <- function(html, description){
tryCatch(
{
html <- read_html(html)
text_1 <- html %>% html_elements(xpath = '//*[@id="js-article__body"]/p') %>% html_text() %>% toString()
text_2 <- html %>% html_elements(xpath = '//*[@class="paywall"]/p') %>% html_text() %>% toString()
# Preporcessing Text
text <- paste(preprocess_text(text_1), preprocess_text(text_2))
if (text == ""){
return(description)
} else{
return(text)
}
},error = {
return("")
}
)
}
prepare_text <- function(publisher, articleURL, description) {
if (is.na(description)){
return("")
}
else{
if (publisher == "Zacks Investment Research") {
return(preprocess_text(scrap_text_zack(articleURL, description)))
} else if (publisher == "MarketWatch") {
return(preprocess_text(scrap_text_marketwatch(articleURL, description)))
} else {
return(preprocess_text(description))
}
}
}
# Wende die prepare_text-Funktion auf jede Zeile im DataFrame an
stock_news$preprocessed_text <- mapply(prepare_text, stock_news$Publisher, stock_news$ArticleURL, stock_news$Description)
scrap_text_zack <- function(html, description){
tryCatch(
{
html <- read_html(html)
text <- html %>% html_elements(xpath = '//*[@id="comtext"]/p[position()<3]') %>% html_text() %>% toString()
if (text == ""){
return(description)
} else{
return(text)
}
}, error={
return("")
}
)
}
scrap_text_marketwatch <- function(html, description){
html <- read_html(html)
text_1 <- html %>% html_elements(xpath = '//*[@id="js-article__body"]/p') %>% html_text() %>% toString()
text_2 <- html %>% html_elements(xpath = '//*[@class="paywall"]/p') %>% html_text() %>% toString()
# Preporcessing Text
text <- paste(preprocess_text(text_1), preprocess_text(text_2))
if (text == ""){
return(description)
} else{
return(text)
}
}
prepare_text <- function(publisher, articleURL, description) {
if (is.na(description)){
return("")
}
else{
if (publisher == "Zacks Investment Research") {
return(preprocess_text(scrap_text_zack(articleURL, description)))
} else if (publisher == "MarketWatch") {
return(preprocess_text(scrap_text_marketwatch(articleURL, description)))
} else {
return(preprocess_text(description))
}
}
}
# Wende die prepare_text-Funktion auf jede Zeile im DataFrame an
stock_news$preprocessed_text <- mapply(prepare_text, stock_news$Publisher, stock_news$ArticleURL, stock_news$Description)
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(tidyverse)
library(readr)
library(tidytext)
library(textdata)
library(syuzhet)
library(quanteda)
library(stopwords)
library(topicmodels)
library(quanteda.textplots)
library(ldatuning)
library(rvest)
options(scipen=999)
rm(list=ls())
libraries = c("tidyverse", "dplyr", "readr", "textdata", "tidytext", "syuzhet", "stopwords", "quanteda.textplots", "topicmodels", "quanteda", "ldatuning", "rvest")
lapply(libraries, function(x) if (!(x %in% installed.packages())) {
install.packages(x)
})
lapply(libraries, library, quietly = TRUE, character.only = TRUE)
set.seed(8)
setwd("C:/Users/Student/polygon")
data_raw <- read_csv("stock_news2.csv")
stock_news <- data_raw %>% drop_na(ArticleURL)
# Define a function to preprocess the text
preprocess_text <- function(text) {
# Convert text to lower case
text <- tolower(text)
# Remove non-ASCII characters, including emojis and emoticons
text <- iconv(text, "UTF-8", "ASCII", sub="")
# Remove numbers
text <- gsub("\\d+", "", text)
# Remove punctuation
text <- gsub("[[:punct:]]", "", text)
# Remove whitespace, including 3 consecutive spaces
text <- gsub("\\s{3}", " ", text)
text <- gsub("\\s+", " ", text)
# Remove leading and trailing white spaces
text <- trimws(text)
# Remove stopwords and other words to be removed
words_to_remove <- c("the", "and", "in", "to", "a", "of")
words_to_remove_pattern <- paste0("\\b(", paste(words_to_remove, collapse = "|"), ")\\b")
text <- gsub(words_to_remove_pattern, "", text, ignore.case = TRUE)
# Return the preprocessed text
return(text)
}
scrap_text_zack <- function(html, description){
tryCatch(
{
html <- read_html(html)
text <- html %>% html_elements(xpath = '//*[@id="comtext"]/p[position()<3]') %>% html_text() %>% toString()
if (text == ""){
return(description)
} else{
return(text)
}
}, error={
return("")
}
)
}
scrap_text_marketwatch <- function(html, description){
html <- read_html(html)
text_1 <- html %>% html_elements(xpath = '//*[@id="js-article__body"]/p') %>% html_text() %>% toString()
text_2 <- html %>% html_elements(xpath = '//*[@class="paywall"]/p') %>% html_text() %>% toString()
# Preporcessing Text
text <- paste(preprocess_text(text_1), preprocess_text(text_2))
if (text == ""){
return(description)
} else{
return(text)
}
}
prepare_text <- function(publisher, articleURL, description) {
if (is.na(description)){
return("")
}
tryCatch(
{
if (publisher == "Zacks Investment Research") {
return(preprocess_text(scrap_text_zack(articleURL, description)))
} else if (publisher == "MarketWatch") {
return(preprocess_text(scrap_text_marketwatch(articleURL, description)))
} else {
return(preprocess_text(description))
}
}, error={
return(preprocess_text(description))
}
)
}
# Wende die prepare_text-Funktion auf jede Zeile im DataFrame an
stock_news$preprocessed_text <- mapply(prepare_text, stock_news$Publisher, stock_news$ArticleURL, stock_news$Description)
View(stock_news)
scrap_text_zack <- function(html, description){
tryCatch(
{
html <- read_html(html)
text <- html %>% html_elements(xpath = '//*[@id="comtext"]/p[position()<3]') %>% html_text() %>% toString()
if (text == ""){
return(description)
} else{
return(text)
}
}, error={
return("")
}
)
}
scrap_text_marketwatch <- function(html, description){
html <- read_html(html)
text_1 <- html %>% html_elements(xpath = '//*[@id="js-article__body"]/p') %>% html_text() %>% toString()
text_2 <- html %>% html_elements(xpath = '//*[@class="paywall"]/p') %>% html_text() %>% toString()
# Preporcessing Text
text <- paste(text_1, text_2)
if (text == ""){
return(description)
} else{
return(text)
}
}
prepare_text <- function(publisher, articleURL, description) {
if (is.na(description)){
return("")
}
tryCatch(
{
if (publisher == "Zacks Investment Research") {
return(preprocess_text(scrap_text_zack(articleURL, description)))
} else if (publisher == "MarketWatch") {
return(preprocess_text(scrap_text_marketwatch(articleURL, description)))
} else {
return(preprocess_text(description))
}
}, error={
return(preprocess_text(description))
}
)
}
# Wende die prepare_text-Funktion auf jede Zeile im DataFrame an
stock_news$preprocessed_text <- mapply(prepare_text, stock_news$Publisher, stock_news$ArticleURL, stock_news$Description)
View(stock_news)
#Select text column and calculate sentiment scores.
stock_news$sentiment <- "NA"
stock_news$sentiment <- syuzhet::get_sentiment(stock_news$preprocessed_text, method="syuzhet", lang="english")
News_aggregated <- stock_news %>%
group_by(Ticker, Weeknumber) %>%
summarise(mean_sentiment = mean(sentiment, na.rm = TRUE)) %>%
arrange(Ticker, Weeknumber)
View(News_aggregated)
setwd("C:/Thesis")
knitr::opts_chunk$set(echo = TRUE)
## Import data
# We set the working directory and we call the data that we are going to use. Please import the csv file.
setwd("C:/Thesis")
churn_data <- read_csv("Data/cases.csv")
# Load libraries
# We call all the libraries that we are going to use
library(readr)
library(dplyr)
library(caret)
library(DescTools)
library(pROC)
library(ROCR)
library(randomForest)
library(rpart)
library(rpart.plot)
library (dlookr)
options(scipen=999)
## Import data
# We set the working directory and we call the data that we are going to use. Please import the csv file.
setwd("C:/Thesis")
churn_data <- read_csv("Data/cases.csv")
data <- churn_data # We make a copy from the original dataset and work on the copy
View(data)
# Checking dimensions and structure of the data
dim(data)
str(data)
head(data)
# We check the summary of all variables included. Here might be able to identify some data quality issues.
summary(data)
# Examine if there are missing values
diagnose(data)
# Examine if there are missing values
diagnose(data)
str(data)
head(data)
#hello world
x <- 1+1
print (x)
data <- cars
data
plot (data$speed~data$dist)
# Calculate the correlations between the variables
corr <- cor(data[-1])
corr
# Visualization methods - full matrix
corrplot(corr) # default method is the circle
# Load libraries
# We call all the libraries that we are going to use
library(corrr)
# Calculate the correlations between the variables
corr <- cor(data[-1])
corr
# Visualization methods - full matrix
corrplot(corr) # default method is the circle
library(corrplot)
# Calculate the correlations between the variables
corr <- cor(data[-1])
corr
# Visualization methods - full matrix
corrplot(corr) # default method is the circle
corrplot(corr, method = "pie") # we can also change the method
corrplot(corr, method = "color")
corrplot(corr, method = "number")
# Getting started
knitr::opts_chunk$set(echo = TRUE)
libraries = c("readr", "ggplot2","Boruta", "dlookr", "ROCR", "caret", "pROC", "dplyr", "ROSE", "corrplot", "DescTools", "ggpubr", "tidyverse", "RColorBrewer", "ggcorrplot", "PerformanceAnalytics", "corrr", "networkD3", "reshape", "knitr")
lapply(libraries, function(x) if (!(x %in% installed.packages())) {
install.packages(x)
})
lapply(libraries, library, quietly = TRUE, character.only = TRUE)
rm(list=ls())
set.seed(7)
setwd("C:/Users/Student/OneDrive - Berner Fachhochschule/S5/SBD2")
data <- read_csv("data_final_stand.csv")
data <- data.frame(data)
head(data)
tail(data)
str(data)
apply(data, 2, function(x) any(is.na(x)))
apply(data, 2, function(x) any(is.na(x)))
knitr::opts_chunk$set(echo = TRUE)
# Load libraries
# We call all the libraries that we are going to use
library(corrr)
library(corrplot)
library(readr)
library(dplyr)
library(caret)
library(DescTools)
library(pROC)
library(ROCR)
library(randomForest)
library(rpart)
library(rpart.plot)
library (dlookr)
options(scipen=999)
## Import data
# We set the working directory and we call the data that we are going to use. Please import the csv file.
rm(list=ls())
set.seed(7)
setwd("C:/Thesis")
churn_data <- read_csv("Data/cases.csv")
data <- churn_data # We make a copy from the original dataset and work on the copy
# Checking dimensions and structure of the data
dim(data)
str(data)
head(data)
# We check the summary of all variables included. Here might be able to identify some data quality issues.
summary(data)
# Examine if there are missing values
diagnose(data)
apply(data, 2, function(x) any(is.na(x)))
# Getting started
knitr::opts_chunk$set(echo = TRUE)
libraries = c("readr", "ggplot2","Boruta", "dlookr", "ROCR", "caret", "pROC", "dplyr", "ROSE", "corrplot", "DescTools", "ggpubr", "tidyverse", "RColorBrewer", "ggcorrplot", "PerformanceAnalytics", "corrr", "networkD3", "reshape", "knitr")
lapply(libraries, function(x) if (!(x %in% installed.packages())) {
install.packages(x)
})
lapply(libraries, library, quietly = TRUE, character.only = TRUE)
rm(list=ls())
set.seed(7)
setwd("C:/Users/Student/OneDrive - Berner Fachhochschule/S5/SBD2")
data <- read_csv("data_final_stand.csv")
data <- data.frame(data)
head(data)
tail(data)
str(data)
summary(data)
apply(data, 2, function(x) any(is.na(x)))
View(data)
# In the next step, we set binary variables as factor.
data[,c(20:24)] <- apply(data[,c(20:24)], 2, function(x) as.factor(as.character(x)))
knitr::opts_chunk$set(echo = TRUE)
library(DescTools)
library(xgboost)
install.packages("xgboost")
library(DescTools)
library(xgboost)
library(caret)
library(dplyr)
library(tidyverse)
library(mltools)
install.packages("mltools")
library(DescTools)
library(xgboost)
library(caret)
library(dplyr)
library(tidyverse)
library(mltools)
library(reshape2)
library(data.table)
library(pracma)
install.packages("pracma")
library(DescTools)
library(xgboost)
library(caret)
library(dplyr)
library(tidyverse)
library(mltools)
library(reshape2)
library(data.table)
library(pracma)
library(rsample)
install.packages("rsample")
library(DescTools)
library(xgboost)
library(caret)
library(dplyr)
library(tidyverse)
library(mltools)
library(reshape2)
library(data.table)
library(pracma)
library(rsample)
library(PRROC)
install.packages("PRROC")
install.packages("PRROC")
install.packages("e1071")
install.packages("dlookr")
install.packages("pROC")
install.packages("ROCR")
install.packages("nnet")
install.packages("e1071")
install.packages("pROC")
knitr::opts_chunk$set(echo = TRUE)
library(DescTools)
library(xgboost)
library(caret)
library(dplyr)
library(tidyverse)
library(mltools)
library(reshape2)
library(data.table)
library(pracma)
library(rsample)
library(PRROC)
library(e1071)
library(dlookr)
library(pROC)
library(ROCR)
library(nnet)
options(scipen=999)
## Import data
# We set the working directory and we call the data that we are going to use. Please import the csv file.
rm(list=ls())
set.seed(7)
setwd("C:/Thesis")
churn_data <- read_csv("Data/cases.csv")
data <- churn_data # We make a copy from the original dataset and work on the copy
# Let's run basic check on the dataset
summary(data)
str(data)
table(data$Attrition)
PercTable(data)
View(data)
