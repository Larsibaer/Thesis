filter(field == field_name) %>%
arrange(update_time) %>%
group_by(set) %>%
mutate(
time_diff_previous = as.numeric(difftime(update_time, lag(update_time, default = first(opened)), units = "hours")),
sum_time_diff = sum(time_diff_previous, na.rm = TRUE),
percentage_time_diff = ifelse(is.na(time_diff_previous) & n() == 1, 100, time_diff_previous / sum_time_diff * 100)
) %>%
filter(!is.na(old)) %>%
select(set, old, percentage_time_diff) %>%
ungroup()
}
# Apply the function for different fields
data_state <- calculate_time_diff(data, "state")
data_assignment_group <- calculate_time_diff(data, "assignment_group")
# One-hot encode specified columns
one_hot_encode <- function(df, columns) {
if(!is.character(columns)) stop("columns argument must be a character vector specifying column names to encode")
if(!all(columns %in% names(df))) stop("Not all specified columns exist in the dataframe")
result_df <- df[, !(names(df) %in% columns)]
for(column in columns) {
temp_df <- data.frame(df[[column]], stringsAsFactors = FALSE)
colnames(temp_df) <- column
encoded_matrix <- model.matrix(~ . - 1, data = temp_df)
encoded_df <- data.frame(encoded_matrix)
colnames(encoded_df) <- gsub("X", column, colnames(encoded_df))
result_df <- cbind(result_df, encoded_df)
}
return(cbind(df[1], result_df))
}
# Apply one-hot encoding to different fields
encoded_df_state <- one_hot_encode(data_state, "old")
encoded_df_assignment_group <- one_hot_encode(data_assignment_group, "old")
# Remove unnecessary columns and replace values with percentages
encoded_df_state <- encoded_df_state[-2]
encoded_df_assignment_group <- encoded_df_assignment_group[-2]
replace_with_percentage <- function(df, percentage_col) {
df[, -c(1, 2)] <- df[, -c(1, 2)] * df[, percentage_col]
df[, -1] <- round(df[, -1], 2)
return(df)
}
encoded_df_state <- replace_with_percentage(encoded_df_state, "percentage_time_diff")
encoded_df_assignment_group <- replace_with_percentage(encoded_df_assignment_group, "percentage_time_diff")
# Summarize grouped dataframes
summarize_grouped_df <- function(df) {
df %>%
group_by(set) %>%
summarise(across(-1, sum, na.rm = TRUE)) %>%
ungroup()
}
grouped_encoded_df_state <- summarize_grouped_df(encoded_df_state)
grouped_encoded_df_assignment_group <- summarize_grouped_df(encoded_df_assignment_group)
# Replace column names
replace_column_names <- function(df, prefix) {
colnames(df) <- gsub("old", prefix, colnames(df))
return(df)
}
grouped_encoded_df_state <- replace_column_names(grouped_encoded_df_state, "prop_state_")
grouped_encoded_df_assignment_group <- replace_column_names(grouped_encoded_df_assignment_group, "prop_group_")
# Merge all dataframes
final_df <- Reduce(function(x, y) merge(x, y, by = "set", all = TRUE),
list(grouped_encoded_df_state, grouped_encoded_df_assignment_group))
# Rename 'set' to 'number'
colnames(final_df)[1] <- "number"
View(final_df)
knitr::opts_chunk$set(echo = TRUE, results = "hide")
# Load necessary libraries
library(tidyverse)
library(quanteda)
library(stopwords)
library(topicmodels)
library(tidytext)
library(quanteda.textplots)
options(scipen=999)
# Clear the workspace
rm(list=ls())
# Set working directory
setwd("C:/Thesis")
# Read CSV file
cases_data <- read_csv("Data/TopicModel_VectorizedText_description.csv", locale = locale(encoding = "UTF-8"))
descriptions <- cases_data
# Filter descriptions based on minimum text length
descriptions <- subset(descriptions, nchar(descriptions$description) > 100)
# Remove NAs in description
descriptions <- descriptions[!is.na(descriptions$description),]
# Remove unnecessary hyphens
descriptions$description <- gsub("-", " ", descriptions$description)
# Tokenize text and apply basic preprocessing
tokens <- tokens(descriptions$description,
remove_punct = TRUE,
remove_symbols = TRUE,
remove_numbers = TRUE,
remove_url = TRUE,
remove_separators = TRUE)
# Create n-grams
tokens <- tokens_ngrams(tokens, n = 1:2)
# Create Document-Feature Matrix
myDfm <- dfm(tokens)
# Create LDA model (specify number of topics)
descriptions_lda <- LDA(myDfm, k = 7, control = list(seed = 123))
topics <- as.data.frame(terms(descriptions_lda, 50))
# Convert to tidy format for visualization
descriptions_lda_td <- tidy(descriptions_lda)
# Rename topic column values
descriptions_lda_td$topic <- factor(descriptions_lda_td$topic)
descriptions_lda_td$topic <- factor(descriptions_lda_td$topic, labels = c(
"Network Server", "Performance and Response Issues", "VDI and Hosted Desktop",
"Authentication and Accounts", "Office Applications", "Printing and Drive",
"Support Infrastructure"))
# Save the tidy LDA results
write_csv(descriptions_lda_td, "Data/topicModel_descriptions_lda_td.csv")
# Extract top terms per topic
top_terms <- descriptions_lda_td %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
# Visualize top terms and their loadings
top_terms %>%
mutate(term = reorder(term, beta)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()
# Link Results to Metadata
# Get topic distributions for each document
tmResult <- posterior(descriptions_lda)
theta <- tmResult$topics
lda_results <- cbind(descriptions, theta)
# Add missing rows from cases_data with topic values set to 0
lda_results <- lda_results[-2] %>%
full_join(cases_data, by = "number")
# Fill NA values with 0
lda_results[is.na(lda_results)] <- 0
# Remove last column
lda_results <- lda_results[-ncol(lda_results)]
# Rename columns
colnames(lda_results) <- c("number", "topic_network_server", "topic_performance_responseIssues",
"topic_vdi_hostedDesktop", "topic_authentication_accounts",
"topic_officeApplications", "topic_printing_drive", "topic_support_infrastructure")
# Save the final dataframe
# write_csv(lda_results, "Data/topicModel_description.csv")
# Load ldatuning library
library(ldatuning)
# Calculate different metrics to estimate the best number of topics for LDA model
result <- FindTopicsNumber(
myDfm,
topics = seq(from = 2, to = 10, by = 1),
metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
method = "Gibbs",
control = list(seed = 77),
mc.cores = 2L,
verbose = TRUE)
# Read additional text data
close_notes <- read_csv("Data/TopicModel_VectorizedText_close_notes.csv", locale = locale(encoding = "UTF-8"))
cause <- read_csv("Data/TopicModel_VectorizedText_cause.csv", locale = locale(encoding = "UTF-8"))
# Combine all text information
all_text <- merge(close_notes, cause, by = "number", all = TRUE)
all_text <- merge(all_text, descriptions, by = "number", all = TRUE)
# Concatenate all text fields into one
all_text$all_text <- paste(all_text$close_notes, all_text$cause, all_text$description, sep = " ")
# Replace "NA" text with empty strings and trim white spaces
all_text$all_text <- gsub("NA", "", all_text$all_text)
all_text$all_text <- trimws(all_text$all_text)
# Fill empty text with NA
all_text$all_text[all_text$all_text == ""] <- NA
# Remove individual text columns
all_text <- all_text %>%
select(-close_notes, -cause, -description)
# Process Combined Text Data
# Remove NAs in combined text data
all_text <- all_text[!is.na(all_text$all_text),]
# Tokenize combined text data
new_tokens <- tokens(all_text$all_text,
remove_punct = TRUE,
remove_symbols = TRUE,
remove_numbers = TRUE,
remove_url = TRUE,
remove_separators = TRUE)
# Create n-grams for combined text data
new_tokens <- tokens_ngrams(new_tokens, n = 1:2)
# Create Document-Feature Matrix for new data
new_myDfm <- dfm(new_tokens, verbose = FALSE)
# Apply the trained LDA model to the new data
new_descriptions_lda <- LDA(new_myDfm, model = descriptions_lda)
# Get topic distributions for the new data
tmResult <- posterior(new_descriptions_lda)
theta <- tmResult$topics
lda_results2 <- cbind(all_text, theta)
# Clean up temporary variables
rm(theta, new_descriptions_lda_td, tmResult, top_terms, tokens)
# Add missing rows from cases_data with topic values set to 0
lda_results2 <- lda_results2[-2]
lda_results2[is.na(lda_results2)] <- 0
# Rename columns
colnames(lda_results2) <- c("number", "topic_network_server", "topic_performance_responseIssues",
"topic_vdi_hostedDesktop", "topic_authentication_accounts",
"topic_officeApplications", "topic_printing_drive", "topic_support_infrastructure")
# Save the final dataframe
# write_csv(lda_results2, "Data/topicModel_allText.csv")
View(lda_results2)
# Extract top terms per topic
top_terms <- descriptions_lda_td %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
# Convert top_terms topic to string
top_terms$topic <- as.character(top_terms$topic)
topic_labels <- c(
"Network & Server", "Performance & Response Issues", "VDI & Hosted Desktop",
"Authentication & Accounts", "Office Applications", "Printing & Drive",
"Support Infrastructure")
# Assign labels to top terms
for (i in 1:length(topic_labels)) {
top_terms$topic[((i - 1) * 10 + 1):(i * 10)] <- topic_labels[i]
}
# Visualize top terms and their loadings
top_terms %>%
mutate(term = reorder(term, beta)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(dplyr)
library(cluster)
library(factoextra)
library(mlbench)
library(tidyr)
library(readr)
library(FactoMineR)
library(dbscan)
library(fpc)
library(data.table)
library(tidyverse)
options(scipen=999)
# Import data
rm(list=ls())
set.seed(7)
setwd("C:/Thesis")
cases_data <- read_csv("Data/data.csv", locale = locale(encoding = "UTF-8"))
data <- cases_data # Make a copy from the original dataset
# Get Column position of description to close notes
description_index <- which(colnames(data) == "description")
cause_index <- which(colnames(data) == "cause")
close_notes_index <- which(colnames(data) == "close_notes")
assignmet_group_index <- which(colnames(data) == "assignment_group")
case_cause_index <- which(colnames(data) == "case_cause")
resolution_code_index <- which(colnames(data) == "resolution_code")
# Remove the columns
data <- data[,-c(description_index, cause_index, close_notes_index, assignmet_group_index, case_cause_index, resolution_code_index)]
# Add weekday column
data$weekday <- weekdays(data$opened)
tfidf_1 <- read_csv("Data/topicModel_description.csv")
one_hot_encode <- function(df, columns) {
if(!is.character(columns)) {
stop("columns argument must be a character vector specifying column names to encode")
}
if(!all(columns %in% names(df))) {
stop("Not all specified columns exist in the dataframe")
}
result_df <- df[, !(names(df) %in% columns)]
for(column in columns) {
temp_df <- data.frame(df[[column]], stringsAsFactors = FALSE)
colnames(temp_df) <- column
encoded_matrix <- model.matrix(~ . - 1, data = temp_df)
encoded_df <- data.frame(encoded_matrix)
colnames(encoded_df) <- gsub("X", column, colnames(encoded_df))
result_df <- cbind(result_df, encoded_df)
}
return(cbind(df[1], result_df))
}
data <- select(data, number, created_by_group, impact, priority, urgency, weekday)
encoded_df_withCaseNumber <- one_hot_encode(data, c("created_by_group", "impact", "priority", "urgency", "weekday"))
encoded_df <- encoded_df_withCaseNumber %>% select(-number)
encoded_df <- cbind(encoded_df, tfidf_1[-1])
encoded_df <- scale(encoded_df)
# K-means Clustering
set.seed(72)
k <- 3
kmeans_model <- kmeans(encoded_df, centers = k)
elbow_data <- data.frame(
k = 1:15,
total_withinss = numeric(15)
)
for(i in 1:15) {
set.seed(123456)
km <- kmeans(encoded_df, centers = i)
elbow_data[i, "total_withinss"] <- km$tot.withinss
}
ggplot(elbow_data, aes(x = k, y = total_withinss)) +
geom_line() +
geom_point() +
scale_x_continuous(breaks = seq(1, 15, 1)) +
labs(title = "Elbow Method", x = "Number of Clusters",
y = "Total Within-Cluster Sum of Squares") +
theme_minimal()
k <- 8
set.seed(123456)
kmeans_model <- kmeans(encoded_df, centers = k)
pca_fit <- PCA(encoded_df, graph = FALSE)
pc1 <- pca_fit$ind$coord[, 1]
pc2 <- pca_fit$ind$coord[, 2]
cluster_data <- data.frame(cluster = as.factor(kmeans_model$cluster), pc1, pc2)
data_scaled$cluster <- as.factor(kmeans_model$cluster)
k <- 8
set.seed(123456)
kmeans_model <- kmeans(encoded_df, centers = k)
pca_fit <- PCA(encoded_df, graph = FALSE)
pc1 <- pca_fit$ind$coord[, 1]
pc2 <- pca_fit$ind$coord[, 2]
cluster_data <- data.frame(cluster = as.factor(kmeans_model$cluster), pc1, pc2)
data$cluster <- as.factor(kmeans_model$cluster)
table(data_scaled$cluster)
k <- 8
set.seed(123456)
kmeans_model <- kmeans(encoded_df, centers = k)
pca_fit <- PCA(encoded_df, graph = FALSE)
pc1 <- pca_fit$ind$coord[, 1]
pc2 <- pca_fit$ind$coord[, 2]
cluster_data <- data.frame(cluster = as.factor(kmeans_model$cluster), pc1, pc2)
data$cluster <- as.factor(kmeans_model$cluster)
table(data$cluster)
data_subset <- data_scaled %>%
group_by(cluster) %>%
slice_head(n = 2) %>%
ungroup()
k <- 8
set.seed(123456)
kmeans_model <- kmeans(encoded_df, centers = k)
pca_fit <- PCA(encoded_df, graph = FALSE)
pc1 <- pca_fit$ind$coord[, 1]
pc2 <- pca_fit$ind$coord[, 2]
cluster_data <- data.frame(cluster = as.factor(kmeans_model$cluster), pc1, pc2)
data$cluster <- as.factor(kmeans_model$cluster)
table(data$cluster)
data_subset <- data %>%
group_by(cluster) %>%
slice_head(n = 2) %>%
ungroup()
# PCA Analysis
pca_fit <- PCA(encoded_df, graph = FALSE)
pc1 <- pca_fit$ind$coord[, 1]
pc2 <- pca_fit$ind$coord[, 2]
summary(pc1)
summary(pc2)
pca_fit$eig
sum_PCA_2 <- pca_fit$eig[2, 3]
cat("Cumulative variance explained by PC1 and PC2:", round(sum_PCA_2, 2), "\n")
sum_PCA_3 <- pca_fit$eig[3, 3]
cat("Cumulative variance explained by PC1 until PC3:", round(sum_PCA_3, 2), "\n")
pca_fit$var$contrib
ggplot(cluster_data, aes(pc1, pc2, color = cluster)) +
geom_point() +
labs(color = "Cluster") +
theme_minimal()
# Analyze Clusters
final_data$cluster <- as.factor(final_data$cluster)
final_data <- merge(cases_data, data_scaled[c('number','cluster')], by = "number", all.x = FALSE)
final_data <- merge(cases_data, data[c('number','cluster')], by = "number", all.x = FALSE)
# Analyze Clusters
final_data$cluster <- as.factor(final_data$cluster)
summary_stats <- final_data %>%
group_by(cluster) %>%
summarise_all(funs(mean = mean(., na.rm = TRUE), sd = sd(., na.rm = TRUE)))
ggplot(final_data, aes(x = cluster, y = final_data$openedToClosed)) +
geom_boxplot() +
theme_minimal() +
labs(title = "Distribution of Opened to Closed Across Clusters", y = "Opened to Closed (Hours)", x = "Cluster")
ggplot(final_data, aes(x = cluster, y = final_data$time_worked)) +
geom_boxplot() +
theme_minimal() +
labs(title = "Distribution of Time Worked Across Clusters", y = "Opened to Closed (Hours)", x = "Cluster")
ggplot(final_data, aes(x = cluster, fill = account)) +
geom_bar() +
theme_minimal() +
labs(title = "Distribution of Account Across Clusters", y = "Count", x = "Cluster") +
scale_fill_discrete(name = "Account")
summary(final_data[final_data$cluster == c(1,2,3,4,5,6,8),])
summary(final_data[final_data$cluster == 7,])
# Analyze Clusters
final_data$cluster <- as.factor(final_data$cluster)
summary_stats <- final_data %>%
group_by(cluster) %>%
summarise_all(funs(mean = mean(., na.rm = TRUE), sd = sd(., na.rm = TRUE)))
ggplot(final_data, aes(x = cluster, y = final_data$openedToClosed)) +
geom_boxplot() +
theme_minimal() +
labs(title = "Distribution of Opened to Closed Across Clusters", y = "Opened to Closed (Hours)", x = "Cluster")
summary(final_data[final_data$cluster == c(1,2,3,4,5,6,8),])
summary(final_data[final_data$cluster == 7,])
View(encoded_df)
dbscan_data1 <- dbscan(encoded_df, eps = 3.75, MinPts = 30)
#dbscan_data1 <- dbscan(encoded_df, eps = 3.75, MinPts = 25)
cluster_data <- data.frame(number = data$number, cluster = dbscan_data1$cluster)
# Merge the cluster information back to the original data
final_data <- merge(cases_data, cluster_data, by = "number", all.x = TRUE)
# Count the number of stocks in each cluster
table(final_data$cluster)
#Median of openedToClosed as number
summary(final_data[final_data$cluster == c(1:3,5:12),])
#summary me the distibution of the cluster 4
summary(final_data[final_data$cluster == 4,])
# Assuming 'data' is your dataset and 'cluster' is the column indicating cluster membership
final_data$cluster <- as.factor(final_data$cluster) # Ensure the cluster variable is a factor
# Summary statistics for each cluster
summary_stats <- final_data %>%
group_by(cluster) %>%
summarise_all(funs(mean = mean(., na.rm = TRUE), sd = sd(., na.rm = TRUE)))
# Boxplots for a variable across clusters
ggplot(final_data, aes(x = cluster, y = final_data$openedToClosed)) +
geom_boxplot() +
theme_minimal() +
labs(title = "Distribution of Opened to Closed Across Clusters", y = "Opened to Closed (Hours)", x = "Cluster")
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(dplyr)
library(cluster)
library(factoextra)
library(mlbench)
library(tidyr)
library(readr)
library(FactoMineR)
library(dbscan)
library(fpc)
library(data.table)
library(tidyverse)
options(scipen=999)
# Import data
rm(list=ls())
set.seed(7)
setwd("C:/Thesis")
cases_data <- read_csv("Data/data.csv", locale = locale(encoding = "UTF-8"))
data <- cases_data # Make a copy from the original dataset
# Get Column position of description to close notes
description_index <- which(colnames(data) == "description")
cause_index <- which(colnames(data) == "cause")
close_notes_index <- which(colnames(data) == "close_notes")
assignmet_group_index <- which(colnames(data) == "assignment_group")
case_cause_index <- which(colnames(data) == "case_cause")
resolution_code_index <- which(colnames(data) == "resolution_code")
# Remove the columns
data <- data[,-c(description_index, cause_index, close_notes_index, assignmet_group_index, case_cause_index, resolution_code_index)]
# Add weekday column
data$weekday <- weekdays(data$opened)
tfidf_1 <- read_csv("Data/topicModel_description.csv")
one_hot_encode <- function(df, columns) {
if(!is.character(columns)) {
stop("columns argument must be a character vector specifying column names to encode")
}
if(!all(columns %in% names(df))) {
stop("Not all specified columns exist in the dataframe")
}
result_df <- df[, !(names(df) %in% columns)]
for(column in columns) {
temp_df <- data.frame(df[[column]], stringsAsFactors = FALSE)
colnames(temp_df) <- column
encoded_matrix <- model.matrix(~ . - 1, data = temp_df)
encoded_df <- data.frame(encoded_matrix)
colnames(encoded_df) <- gsub("X", column, colnames(encoded_df))
result_df <- cbind(result_df, encoded_df)
}
return(cbind(df[1], result_df))
}
data <- select(data, number, created_by_group, impact, priority, urgency, weekday)
encoded_df_withCaseNumber <- one_hot_encode(data, c("created_by_group", "impact", "priority", "urgency", "weekday"))
encoded_df <- encoded_df_withCaseNumber %>% select(-number)
encoded_df <- cbind(encoded_df, tfidf_1[-1])
encoded_df <- scale(encoded_df)
dbscan_data1 <- dbscan(encoded_df, eps = 3.75, MinPts = 30)
#dbscan_data1 <- dbscan(encoded_df, eps = 3.75, MinPts = 25)
cluster_data <- data.frame(number = data$number, cluster = dbscan_data1$cluster)
# Merge the cluster information back to the original data
final_data <- merge(cases_data, cluster_data, by = "number", all.x = TRUE)
# Count the number of stocks in each cluster
table(final_data$cluster)
#Median of openedToClosed as number
summary(final_data[final_data$cluster == c(1:3,5:12),])
#summary me the distibution of the cluster 4
summary(final_data[final_data$cluster == 4,])
# PCA Analysis
pca_fit <- PCA(encoded_df, graph = FALSE)
pc1 <- pca_fit$ind$coord[, 1]
pc2 <- pca_fit$ind$coord[, 2]
summary(pc1)
summary(pc2)
pca_fit$eig
sum_PCA_2 <- pca_fit$eig[2, 3]
cat("Cumulative variance explained by PC1 and PC2:", round(sum_PCA_2, 2), "\n")
sum_PCA_3 <- pca_fit$eig[3, 3]
cat("Cumulative variance explained by PC1 until PC3:", round(sum_PCA_3, 2), "\n")
pca_fit$var$contrib
ggplot(cluster_data, aes(pc1, pc2, color = cluster)) +
geom_point() +
labs(color = "Cluster") +
theme_minimal()
final_data <- merge(cases_data, data[c('number','cluster')], by = "number", all.x = FALSE)
# Merge the cluster information back to the original data
final_data <- merge(cases_data, cluster_data, by = "number", all.x = TRUE)
# Analyze Clusters
final_data$cluster <- as.factor(final_data$cluster)
summary_stats <- final_data %>%
group_by(cluster) %>%
summarise_all(funs(mean = mean(., na.rm = TRUE), sd = sd(., na.rm = TRUE)))
ggplot(final_data, aes(x = cluster, y = final_data$openedToClosed)) +
geom_boxplot() +
theme_minimal() +
labs(title = "Distribution of Opened to Closed Across Clusters", y = "Opened to Closed (Hours)", x = "Cluster")
summary(final_data[final_data$cluster == c(1,2,3,4,5,6,8),])
summary(final_data[final_data$cluster == 7,])
