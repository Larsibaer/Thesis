### - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
# OPTIONAL: Specify minimum text length (number of characters)
descriptions <- subset(descriptions, descriptions$description > 100)
# remove NAs in description
descriptions <- descriptions[!is.na(descriptions$description),]
# OPTIONAL: Further selection (select variable to filter)
# descriptions<- subset(descriptions, descriptions$product == "Fitbit Charge 2")
# OPTIONAL: Create random sample
# descriptions <- sample_n(descriptions, 1000)
### - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
### STEP 3: PREPARE TEXT DATA (TEXT PRE-PROCESSING)
### - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
# Remove all "-" where they are not needed
descriptions$description <- gsub("-", " ", descriptions$description)
# Transform words into tokens, select basic text preprocessing steps
tokens <- tokens(descriptions$description,
remove_punct = TRUE,
remove_symbols = TRUE,
remove_numbers = TRUE,
remove_url = TRUE,
remove_separators = TRUE)
# Create n-grams of any length
tokens <- tokens_ngrams(tokens, n = 1:2)
# Create Document-feature-matrix
myDfm <-dfm(tokens)
### - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
### STEP 4: ANALYZE TEXT
### - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
# Create LDA model (specify number of topics)
descriptions_lda <- LDA(myDfm, k = 7, control = list(seed = 123))
topics <- as.data.frame(terms(descriptions_lda, 50))
# Convert into tidy-format to visualize results
descriptions_lda_td <- tidy(descriptions_lda)
# Extract top-terms per topic
top_terms <- descriptions_lda_td %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
# Visualize top-terms and their loadings (can you assign topic labels based on this information?)
top_terms %>%
mutate(term = reorder(term, beta)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()
# Link results to metadata
tmResult <- posterior(descriptions_lda)
theta <- tmResult$topics
lda_results <- cbind(descriptions, theta)
rm (theta,descriptions_lda_td,tmResult,top_terms,tokens)
# add all rows from cases_data to lda_results, where number is missing and add the value 0 to all topics
lda_results <- lda_results[-2] %>%
full_join(cases_data, by = "number")
# fill NA with 0
lda_results[is.na(lda_results)] <- 0
# remove last column
lda_results <- lda_results[-ncol(lda_results)]
# Rename columns
colnames(lda_results) <- c("number", "topic_network_server", "topic_performance_responseIssues", "topic_vdi_hostedDesktop", "topic_authentication_accounts", "topic_officeApplications", "topic_printing_drive", "topic_support_infrastructure")
# Save the final dataframe
write_csv(lda_results, "Data/topicModel_description.csv")
### - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
### STEP 5: GO BACK TO STEP 3 AND 4 TO RECONSIDER PREPROCESSING, STOPWORDS AND THE NUMBER OF TOPICS.
###         ITERATE MULTIPLE TIMES AND OBSERVE HOW RESULTS CHANGE. GOOD LUCK!
### - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
### - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
### OPTIONAL: CHOOSE THE BEST NUMBER OF TOPICS BASED ON A METRIC
### - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
library (ldatuning)
### Calculate different metrics to estimate the most preferable number of topics for LDA model
## Be aware: The procedure is computation intensive
# ldatuning uses parallelism, specify the correct number of CPU cores in mc.core parameter to archive best performance
# Calculate selected metrics
result <- FindTopicsNumber(
myDfm,
topics = seq(from = 2, to = 10, by = 1),
metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
method = "Gibbs",
control = list(seed = 77),
mc.cores = 2L,
verbose = TRUE)
# plot results
FindTopicsNumber_plot(result)
###################################################################################################
### All Text Information
###################################################################################################
close_notes <- read_csv("Data/TopicModel_VectorizedText_close_notes.csv", locale = locale(encoding = "UTF-8"))
cause <- read_csv("Data/TopicModel_VectorizedText_cause.csv", locale = locale(encoding = "UTF-8"))
# Combine all text information
all_text <- merge(close_notes, cause, by = "number", all = TRUE)
all_text <- merge(all_text, descriptions, by = "number", all = TRUE)
# Paste all text information together
all_text$all_text <- paste(all_text$close_notes, all_text$cause, all_text$description, sep = " ")
#Replace Text containing LOG with ''
all_text$all_text <- gsub("NA", "", all_text$all_text)
# Trim white spaces at beginning and end of text
all_text$all_text <- trimws(all_text$all_text)
# Fill empty text with NA
all_text$all_text[all_text$all_text == ""] <- NA
# Remove column close_notes, cause and description
all_text <- all_text[-2]
all_text <- all_text[-2]
all_text <- all_text[-2]
### - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
### STEP 2: LOAD AND SELECT DATA (E.G. AMAZON descriptions)
### - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
# OPTIONAL: Specify minimum text length (number of characters)
all_text <- subset(all_text, all_text$all_text > 100)
# remove NAs in description
all_text <- all_text[!is.na(all_text$all_text),]
# Transform words into tokens, select basic text preprocessing steps
new_tokens <- tokens(all_text$all_text,
remove_punct = TRUE,
remove_symbols = TRUE,
remove_numbers = TRUE,
remove_url = TRUE,
remove_separators = TRUE)
# Create n-grams of any length
new_tokens <- tokens_ngrams(new_tokens, n = 1:2)
# Create Document-feature-matrix for new data
new_myDfm <- dfm(new_tokens, verbose = FALSE)
# Install necessary packages
install.packages(c("tm", "text2vec", "wordcloud"))
# Load the packages
library(tm)
library(text2vec)
library(wordcloud)
# Create a wordcloud from your Document-Feature-Matrix
textplot_wordcloud(myDfm,
min_size = 0.5,
max_size = 4,
min_count = 10,
max_words = 300,
color = "darkblue")
View(myDfm)
# Create a wordcloud from your Document-Feature-Matrix
# Assuming you have `quanteda` and `quanteda.textplots` installed
library(quanteda)
library(quanteda.textplots)
# Extract top terms for each topic
top_terms <- terms(new_descriptions_lda, 50)
# Extract top terms for each topic
top_terms <- terms(descriptions_lda, 50)
# Get number of topics
num_topics <- ntopics(descriptions_lda)
library(topicmodels)
# Get number of topics
num_topics <- ntopics(descriptions_lda)
# Extract top terms for each topic
top_terms <- terms(new_descriptions_lda, 50)
# Extract top terms for each topic
top_terms <- terms(descriptions_lda, 50)
# Determine the number of topics
num_topics <- nrow(top_terms)
# Create a word cloud for each topic
for (i in 1:num_topics) {
# Extract the terms associated with the current topic
topic_terms <- top_terms[i, ]
# Create a temporary document-feature matrix containing only the terms for this topic
temp_dfm <- dfm_select(myDfm, pattern = topic_terms, selection = "keep", valuetype = "fixed")
# Generate word cloud for the topic
textplot_wordcloud(temp_dfm,
min_size = 0.5,
max_size = 4,
min_count = 10,
max_words = 300,
color = "darkblue",
rotation = 0.25)
# Optionally, add a title for clarity
title(paste("Topic", i, "Word Cloud"))
}
# Determine the number of topics
num_topics <- ncol(top_terms)
# Create a word cloud for each topic
for (i in 1:num_topics) {
# Extract the terms associated with the current topic
topic_terms <- top_terms[i, ]
# Create a temporary document-feature matrix containing only the terms for this topic
temp_dfm <- dfm_select(myDfm, pattern = topic_terms, selection = "keep", valuetype = "fixed")
# Generate word cloud for the topic
textplot_wordcloud(temp_dfm,
min_size = 0.5,
max_size = 4,
min_count = 10,
max_words = 300,
color = "darkblue",
rotation = 0.25)
# Optionally, add a title for clarity
title(paste("Topic", i, "Word Cloud"))
}
View(topics)
# Create a word cloud for each topic
for (i in 1:num_topics) {
# Extract the terms associated with the current topic
topic_terms <- top_terms[i, ]
# Create a temporary document-feature matrix containing only the terms for this topic
temp_dfm <- dfm_select(myDfm, pattern = topic_terms, selection = "keep", valuetype = "fixed")
# Generate word cloud for the topic
textplot_wordcloud(temp_dfm,
min_size = 0.5,
max_size = 4,
min_count = 5,
max_words = 300,
color = "darkblue",
rotation = 0.25)
# Optionally, add a title for clarity
title(paste("Topic", i, "Word Cloud"))
}
close_notes <- read_csv("Data/TopicModel_VectorizedText_close_notes.csv", locale = locale(encoding = "UTF-8"))
cause <- read_csv("Data/TopicModel_VectorizedText_cause.csv", locale = locale(encoding = "UTF-8"))
# Combine all text information
all_text <- merge(close_notes, cause, by = "number", all = TRUE)
all_text <- merge(all_text, descriptions, by = "number", all = TRUE)
# Paste all text information together
all_text$all_text <- paste(all_text$close_notes, all_text$cause, all_text$description, sep = " ")
#Replace Text containing LOG with ''
all_text$all_text <- gsub("NA", "", all_text$all_text)
# Trim white spaces at beginning and end of text
all_text$all_text <- trimws(all_text$all_text)
# Fill empty text with NA
all_text$all_text[all_text$all_text == ""] <- NA
# Remove column close_notes, cause and description
all_text <- all_text[-2]
all_text <- all_text[-2]
all_text <- all_text[-2]
# OPTIONAL: Specify minimum text length (number of characters)
all_text <- subset(all_text, all_text$all_text > 100)
# remove NAs in description
all_text <- all_text[!is.na(all_text$all_text),]
# Transform words into tokens, select basic text preprocessing steps
new_tokens <- tokens(all_text$all_text,
remove_punct = TRUE,
remove_symbols = TRUE,
remove_numbers = TRUE,
remove_url = TRUE,
remove_separators = TRUE)
# Create n-grams of any length
new_tokens <- tokens_ngrams(new_tokens, n = 1:2)
# Create Document-feature-matrix for new data
new_myDfm <- dfm(new_tokens, verbose = FALSE)
# Apply the trained LDA model to the new data
new_descriptions_lda <- LDA(new_myDfm, model = descriptions_lda)
# Extract topics distribution for the new data
new_topics <- as.data.frame(terms(new_descriptions_lda, 50))
# Link results to metadata
tmResult <- posterior(new_descriptions_lda)
theta <- tmResult$topics
lda_results2 <- cbind(all_text, theta)
lda_results2 <- lda_results2[-2]
# fill NA with 0
lda_results2[is.na(lda_results)] <- 0
# Rename columns
colnames(lda_results2) <- c("number", "topic_network_server", "topic_performance_responseIssues", "topic_vdi_hostedDesktop", "topic_authentication_accounts", "topic_officeApplications", "topic_printing_drive", "topic_support_infrastructure")
# Load necessary library for text plotting
library(quanteda)
library(quanteda.textplots)
# Extract top terms for each topic
top_terms <- terms(new_descriptions_lda, 50)
# Determine the number of topics
num_topics <- ncol(top_terms)
# Create a word cloud for each topic
for (i in 1:num_topics) {
# Extract the terms associated with the current topic
topic_terms <- top_terms[i, ]
# Create a temporary document-feature matrix containing only the terms for this topic
temp_dfm <- dfm_select(new_myDfm, pattern = topic_terms, selection = "keep", valuetype = "fixed")
# Generate word cloud for the topic
textplot_wordcloud(temp_dfm,
min_size = 0.5,
max_size = 4,
min_count = 10,
max_words = 300,
color = "darkblue",
rotation = 0.25)
# Optionally, add a title for clarity
title(paste("Topic", i, "Word Cloud"))
}
# Extract top terms for each topic
top_terms <- terms(new_descriptions_lda, 50)
# Extract top terms for each topic
top_terms <- terms(new_descriptions_lda, 100)
# Extract top terms for each topic
top_terms <- terms(new_descriptions_lda, 50)
View(top_terms)
View(lda_results2)
View(top_terms)
View(top_terms)
# Rename columns
colnames(top_terms) <- c("number", "topic_network_server", "topic_performance_responseIssues", "topic_vdi_hostedDesktop", "topic_authentication_accounts", "topic_officeApplications", "topic_printing_drive", "topic_support_infrastructure")
# Rename columns
colnames(top_terms) <- c("topic_network_server", "topic_performance_responseIssues", "topic_vdi_hostedDesktop", "topic_authentication_accounts", "topic_officeApplications", "topic_printing_drive", "topic_support_infrastructure")
View(top_terms)
# Rename columns
colnames(top_terms) <- c("Network & Server", "Performance & Response Issues", "VDI & Hosted Desktop", "Authentication & Accounts", "Office Applications", "Printing & Drive", "Support & Infrastructure")
# Determine the number of topics
num_topics <- ncol(top_terms)
# Create a word cloud for each topic
for (i in 1:num_topics) {
# Extract the terms associated with the current topic
topic_terms <- top_terms[i, ]
# Create a temporary document-feature matrix containing only the terms for this topic
temp_dfm <- dfm_select(new_myDfm, pattern = topic_terms, selection = "keep", valuetype = "fixed")
# Generate word cloud for the topic
textplot_wordcloud(temp_dfm,
min_size = 0.5,
max_size = 4,
min_count = 10,
max_words = 300,
color = "darkblue",
rotation = 0.25)
# Optionally, add a title for clarity
title(paste("Topic", i, "Word Cloud"))
}
# Create a word cloud for each topic
for (i in 1:num_topics) {
# Extract the terms associated with the current topic
topic_terms <- top_terms[i, ]
# Create a temporary document-feature matrix containing only the terms for this topic
temp_dfm <- dfm_select(new_myDfm, pattern = topic_terms, selection = "keep", valuetype = "fixed")
# Generate word cloud for the topic
textplot_wordcloud(temp_dfm,
min_size = 0.5,
max_size = 4,
min_count = 1,
max_words = 300,
color = "darkblue",
rotation = 0.25)
# Optionally, add a title for clarity
title(paste("Topic", i, "Word Cloud"))
}
View(top_terms)
View(temp_dfm)
# Create a word cloud for each topic
for (i in 1:num_topics) {
# Extract the terms associated with the current topic
topic_terms <- top_terms[i, ]
# Create a temporary document-feature matrix containing only the terms for this topic
temp_dfm <- dfm_select(new_myDfm, pattern = topic_terms, selection = "keep", valuetype = "glob")
# Generate word cloud for the topic
textplot_wordcloud(temp_dfm,
min_size = 0.5,
max_size = 4,
min_count = 1,
max_words = 300,
color = "darkblue",
rotation = 0.25)
# Optionally, add a title for clarity
title(paste("Topic", i, "Word Cloud"))
}
View(new_descriptions_lda)
View(new_tokens)
# Visualize top-terms and their loadings (can you assign topic labels based on this information?)
top_terms %>%
mutate(term = reorder(term, beta)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()
# Set working directory
setwd("C:/Thesis")
#read json file
cases_data <- read_csv("Data/TopicModel_VectorizedText_description.csv", locale = locale(encoding = "UTF-8"))
descriptions <- cases_data
rm(list=ls())
new("standardGeneric", .Data = function (x, ...)
standardGeneric("topics"), generic = "topics", package = "topicmodels",
group = list(), valueClass = character(0), signature = "x",
default = NULL, skeleton = (function (x, ...)
stop(gettextf("invalid call in method dispatch to '%s' (no default method)",
"topics"), domain = NA))(x, ...))
# Set working directory
setwd("C:/Thesis")
#read json file
cases_data <- read_csv("Data/TopicModel_VectorizedText_description.csv", locale = locale(encoding = "UTF-8"))
descriptions <- cases_data
# OPTIONAL: Specify minimum text length (number of characters)
descriptions <- subset(descriptions, descriptions$description > 100)
# remove NAs in description
descriptions <- descriptions[!is.na(descriptions$description),]
### - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
### STEP 3: PREPARE TEXT DATA (TEXT PRE-PROCESSING)
### - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
# Remove all "-" where they are not needed
descriptions$description <- gsub("-", " ", descriptions$description)
# Transform words into tokens, select basic text preprocessing steps
tokens <- tokens(descriptions$description,
remove_punct = TRUE,
remove_symbols = TRUE,
remove_numbers = TRUE,
remove_url = TRUE,
remove_separators = TRUE)
# Create n-grams of any length
tokens <- tokens_ngrams(tokens, n = 1:2)
# Create Document-feature-matrix
myDfm <-dfm(tokens)
### - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
### STEP 4: ANALYZE TEXT
### - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
# Create LDA model (specify number of topics)
descriptions_lda <- LDA(myDfm, k = 7, control = list(seed = 123))
topics <- as.data.frame(terms(descriptions_lda, 50))
# Convert into tidy-format to visualize results
descriptions_lda_td <- tidy(descriptions_lda)
# Extract top-terms per topic
top_terms <- descriptions_lda_td %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
# Visualize top-terms and their loadings (can you assign topic labels based on this information?)
top_terms %>%
mutate(term = reorder(term, beta)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()
# Rename columns
colnames(lda_results) <- c("number", "topic_network_server", "topic_performance_responseIssues", "topic_vdi_hostedDesktop", "topic_authentication_accounts", "topic_officeApplications", "topic_printing_drive", "topic_support_infrastructure")
View(top_terms)
View(top_terms)
View(top_terms)
# Get first 10 rows of top_terms and overwrite topic with "Network & Server"
top_terms <- as.data.frame(terms(descriptions_lda, 10))
View(top_terms)
View(descriptions_lda_td)
# Extract top-terms per topic
top_terms <- descriptions_lda_td %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
View(top_terms)
# Get first 10 rows of top_terms and overwrite topic with "Network & Server"
top_terms <- as.data.frame(terms(descriptions_lda, 10))
View(top_terms)
# Get first 10 rows of top_terms and overwrite topic with "Network & Server"
top_terms[:10,1] <- "Network & Server"
# Extract top-terms per topic
top_terms <- descriptions_lda_td %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
# Get first 10 rows of top_terms and overwrite topic with "Network & Server"
top_terms[0:10,1] <- "Network & Server"
# Convert top_terms topic to string
top_terms$topic <- as.character(top_terms$topic)
# Get first 10 rows of top_terms and overwrite topic with "Network & Server"
top_terms[0:10,1] <- "Network & Server"
# Get first 10 rows of top_terms and overwrite topic with "Network & Server"
top_terms$topic[0:10] <- "Network & Server"
View(top_terms)
top_terms$topic[0:10] <- "Network & Server"
top_terms$topic[11:20] <- "Performance & Response Issues"
top_terms$topic[21:30] <- "VDI & Hosted Desktop"
top_terms$topic[31:40] <- "Authentication & Accounts"
top_terms$topic[41:50] <- "Office Applications"
top_terms$topic[51:60] <- "Printing & Drive"
top_terms$topic[61:70] <- "Support Infrastructure"
# Visualize top-terms and their loadings (can you assign topic labels based on this information?)
top_terms %>%
mutate(term = reorder(term, beta)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()
knitr::opts_chunk$set(echo = TRUE)
# We start by installing the necessary libraries. Make sure to uncomment and run
# only the libraries which you haven't installed already.
#install.packages("DescTools")
#install.packages("xgboost")
#install.packages("caret")
#install.packages("dplyr")
#install.packages("tidyverse")
#install.packages("mltools")
#install.packages("reshape2")
#install.packages("data.table")
#install.packages("pracma")
#install.packages("rsample")
# install.packages("PRROC")
# install.packages("e1071")
# install.packages("dlookr")
# install.packages("pROC")
# install.packages("ROCR")
# install.packages("nnet")
library(DescTools)
# We start by installing the necessary libraries. Make sure to uncomment and run
# only the libraries which you haven't installed already.
install.packages("DescTools")
install.packages("xgboost")
install.packages("caret")
install.packages("dplyr")
install.packages("tidyverse")
install.packages("mltools")
install.packages("reshape2")
install.packages("data.table")
install.packages("pracma")
install.packages("rsample")
install.packages("PRROC")
install.packages("e1071")
install.packages("dlookr")
install.packages("pROC")
install.packages("ROCR")
install.packages("nnet")
## Import data
# We set the working directory and we call the data that we are going to use. Please import the csv file.
rm(list=ls())
set.seed(7)
setwd("C:/Thesis")
#read json file
cases_data <- read_csv("Data/sn_customerservice_case.csv", locale = locale(encoding = "UTF-8"))
