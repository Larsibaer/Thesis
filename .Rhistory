all.x = TRUE)
# Filter the top 10 assignment groups of number of cases
data_assignment_group_top10 <- data %>%
group_by(ca_assignment_group) %>%
summarise(n = n()) %>%
arrange(desc(n)) %>%
head(10)
# Filter the top 10 assignment groups of number of cases
data_assignment_group_top10 <- data %>%
group_by(assignment_group) %>%
summarise(n = n()) %>%
arrange(desc(n)) %>%
head(10)
View(data)
data <- merge(x = data, y = sla_data,
by.x = c("number", "closed"),
by.y = c("task", "end_time"),
all.x = TRUE)
View(data)
View(sla_data)
View(data)
View(data)
# Filter the top 10 assignment groups of number of cases
data_assignment_group_top10 <- data %>%
group_by(assignment_group) %>%
summarise(n = n()) %>%
arrange(desc(n)) %>%
head(10)
# Filter the top 10 assignment groups of number of cases
data_assignment_group_top10 <- data %>%
group_by(assignment_group) %>%
summarise(n = n()) %>%
arrange(desc(n)) %>%
head(10)
# Filter the top 10 assignment groups of number of cases
data_assignment_group_top10 <- data %>%
group_by(assigned_group) %>%
summarise(n = n()) %>%
arrange(desc(n)) %>%
head(10)
# Filter data, where assignment group is in the top 10
data_assignment_group <- data %>%
filter(assignment_group %in% data_assignment_group_top10$assigned_group))
# Filter the top 10 assignment groups of number of cases
data_assigned_group_top10 <- data %>%
group_by(assigned_group) %>%
summarise(n = n()) %>%
arrange(desc(n)) %>%
head(10)
# Filter data, where assignment group is in the top 10
data_assigned_group <- data %>%
filter(assigned_group %in% data_assigned_group_top10$assigned_group))
View(data_assigned_group_top10)
# Filter the top 10 assignment groups of number of cases
data_assigned_group_top10 <- data %>%
group_by(assigned_group) %>%
summarise(n = n()) %>%
arrange(desc(n)) %>%
head(10)
# Filter data, where assignment group is in the top 10
data_assigned_group <- data %>%
filter(assigned_group %in% data_assigned_group_top10$assigned_group))
# Filter the top 10 assignment groups of number of cases
data_assigned_group_top10 <- data %>%
group_by(assigned_group) %>%
summarise(n = n()) %>%
arrange(desc(n)) %>%
head(10)
# Filter data, where assignment group is in the top 10
data_assigned_group <- data %>%
filter(assigned_group %in% data_assigned_group_top10$assigned_group)
ggplot(data = data_assignment_group) +
geom_bar(mapping = aes(x = assigned_group, fill = assigned_group))
# Filter the top 10 assignment groups of number of cases
data_assigned_group_top10 <- data %>%
group_by(assigned_group) %>%
summarise(n = n()) %>%
arrange(desc(n)) %>%
head(10)
# Filter data, where assignment group is in the top 10
data_assigned_group <- data %>%
filter(assigned_group %in% data_assigned_group_top10$assigned_group)
ggplot(data = data_assigned_group) +
geom_bar(mapping = aes(x = assigned_group, fill = assigned_group))
data <- select(cases_data, number = ca_number, case = ca_case, case_type = ca_u_case_type, case_cause = ca_cause, closed = ca_closed_at, opened = ca_opened_at, account = ca_account, business_service = ca_business_service, business_service_activity = ca_u_business_service_activity, assigned_to = ca_assigned_to, assignment_group = ca_assignment_group, auto_close = ca_auto_close, comments = ca_comments, time_worked = ca_time_worked, reassignment_count = ca_reassignment_count)
# Transforming dates to Timestamp
data$opened <- strptime(data$opened, "%d-%m-%Y %H:%M:%S", tz="UTC")
data <- data %>%
filter(Year(opened) >= Year("2020-01-01"))
# merg data sets
sla_data_clean <- read_csv("Data/task_sla.csv")
# Remove all rows, where sla_data£status is Cancelled
# sla_data <- sla_data %>%
#   filter(stage != "Cancelled")
# new data set, where sla_data$task is dublicated
sla_data_dublicated <- merged_data %>%
filter(duplicated(task))
# merg data sets
sla_data_clean <- read_csv("Data/task_sla.csv")
# Remove all rows, where sla_data£status is Cancelled
# sla_data <- sla_data %>%
#   filter(stage != "Cancelled")
# Merge data and sla_data, where the number and closed of data is the same as task and stop time of sla_data
sla_data <- select(sla_data_clean, task = task, business_percentage = business_percentage, duration = duration, end_time = end_time)
#remove duplicates of sla_data$task
sla_data <- sla_data %>%
distinct(task, .keep_all = TRUE)
data <- merge(x = data, y = sla_data,
by.x = c("number", "closed"),
by.y = c("task", "end_time"),
all.x = TRUE)
# Filter the top 10 assignment groups of number of cases
data_assignment_group_top10 <- data %>%
group_by(assignment_group) %>%
summarise(n = n()) %>%
arrange(desc(n)) %>%
head(10)
# Filter data, where assignment group is in the top 10
data_assignment_group <- data %>%
filter(assignment_group %in% data_assignment_group_top10$assignment_group)
ggplot(data = data_assignment_group) +
geom_bar(mapping = aes(x = assignment_group, fill = assignment_group))
ggplot(data = data) +
geom_point(
mapping = aes(x = time_worked, y = reassignment_count, color = assignment_group)) +
geom_smooth(mapping = aes(x = time_worked, y = reassignment_count)) +
facet_wrap(~ assignment_group, nrow = 5)
ggplot(data = data, mapping = aes(x = time_worked, y = reassigment_count)) +
geom_point(mapping = aes(color = assignment_group)) +
geom_smooth()
ggplot(data = data, mapping = aes(x = data$time_worked, y = data$reassignment_count)) +
geom_point(mapping = aes(color = assignment_group)) +
geom_smooth()
ggplot(data = data, mapping = aes(x = data$time_worked, y = data$reassignment_count)) +
geom_point(mapping = aes(color = assignment_group, size = data$business_percentage)) +
geom_smooth()
# Save the data
write.csv(data, "Data/data.csv")
knitr::opts_chunk$set(echo = TRUE)
## Import data
set.seed(7)
setwd("C:/Thesis")
#read json file
cases_data <- read_csv("Data/data.csv")
data <- cases_data # We make a copy from the original dataset and work on the copy
## Import data
set.seed(7)
setwd("C:/Thesis")
#read json file
cases_data <- read_csv("Data/data.csv")
data <- cases_data # We make a copy from the original dataset and work on the copy
View(data)
# OPTIONAL: Specify minimum text length (number of characters)
text <- subset(text, data$case > 100)
library(tidyverse)
library(quanteda)
library(stopwords)
library(topicmodels)
library(tidytext)
library(quanteda.textplots)
options(scipen=999)
## Import data
set.seed(7)
setwd("C:/Thesis")
#read json file
cases_data <- read_csv("Data/data.csv")
data <- cases_data # We make a copy from the original dataset and work on the copy
# OPTIONAL: Specify minimum text length (number of characters)
text <- subset(text, data$case > 100)
### - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
### STEP 3: PREPARE TEXT DATA (TEXT PRE-PROCESSING)
### - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
# Define additional stopwords
extended_stopwords <- c("fitbit", "charge", "love", "one", "like", "get", "use", "can")
# Transform words into tokens, select basic text preprocessing steps
tokens <- tokens(data$case,
remove_punct = TRUE,
remove_symbols = TRUE,
remove_numbers = TRUE,
remove_url = TRUE,
remove_separators = TRUE)
# remove stopwords
tokens <- tokens_select(tokens, pattern = c(stopwords("german"), extended_stopwords), selection = "remove")
# transform to lowercase
tokens <- tokens_tolower(tokens)
# Stem all words
tokens <-tokens_wordstem(tokens)
# Create n-grams of any length
tokens <- tokens_ngrams(tokens, n = 1:2)
# Create Document-feature-matrix
myDfm <-dfm(tokens)
View(myDfm)
View(tokens)
### - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
### STEP 4: ANALYZE TEXT
### - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
# Create LDA model (specify number of topics)
reviews_lda <- LDA(myDfm, k = 3, control = list(seed = 1111))
View(myDfm)
#read json file
cases_data <- read_csv("Data/u_cases_thesis.csv")
View(cases_data)
View(cases_data)
data <- select(cases_data, number = ca_number, case = ca_case, description = ca_description, case_type = ca_u_case_type, case_cause = ca_cause, due_date = ca_due_date, first_response_time = ca_first_respnse_time, closed = ca_closed_at, opened = ca_opened_at, account = ca_account, contact = ca_contact, created_by = ca_sys_created_by, business_service = ca_business_service, business_service_activity = ca_u_business_service_activity, assigned_to = ca_assigned_to, assignment_group = ca_assignment_group, auto_close = ca_auto_close, time_worked = ca_time_worked, reassignment_count = ca_reassignment_count, impact = ca_impact, priority = ca_priority, urgency = ca_urgency, escalation = ca_escalation, sla_breached = sla_has_breached, comments = ca_comments, case_cause = ca_u_case_cause, cause = ca_cause, close_notes = ca_close_notes, resolution_code = ca_resolution_code, problem = ca_problem)
data <- select(cases_data, number = ca_number, case = ca_case, description = ca_description, case_type = ca_u_case_type, case_cause = ca_cause, due_date = ca_due_date, first_response_time = ca_first_response_time, closed = ca_closed_at, opened = ca_opened_at, account = ca_account, contact = ca_contact, created_by = ca_sys_created_by, business_service = ca_business_service, business_service_activity = ca_u_business_service_activity, assigned_to = ca_assigned_to, assignment_group = ca_assignment_group, auto_close = ca_auto_close, time_worked = ca_time_worked, reassignment_count = ca_reassignment_count, impact = ca_impact, priority = ca_priority, urgency = ca_urgency, escalation = ca_escalation, sla_breached = sla_has_breached, comments = ca_comments, case_cause = ca_u_case_cause, cause = ca_cause, close_notes = ca_close_notes, resolution_code = ca_resolution_code, problem = ca_problem)
data <- select(cases_data, number = ca_number, case = ca_case, description = ca_description, case_type = ca_u_case_type, due_date = ca_due_date, first_response_time = ca_first_response_time, closed = ca_closed_at, opened = ca_opened_at, account = ca_account, contact = ca_contact, created_by = ca_sys_created_by, business_service = ca_business_service, business_service_activity = ca_u_business_service_activity, assigned_to = ca_assigned_to, assignment_group = ca_assignment_group, auto_close = ca_auto_close, time_worked = ca_time_worked, reassignment_count = ca_reassignment_count, impact = ca_impact, priority = ca_priority, urgency = ca_urgency, escalation = ca_escalation, sla_breached = sla_has_breached, comments = ca_comments, case_cause = ca_u_case_cause, cause = ca_cause, close_notes = ca_close_notes, resolution_code = ca_resolution_code, problem = ca_problem)
# Transforming dates to Timestamp
data$opened <- strptime(data$opened, "%d-%m-%Y %H:%M:%S", tz="UTC")
data <- data %>%
filter(Year(opened) >= Year("2020-01-01"))
# merg data sets
sla_data_clean <- read_csv("Data/task_sla.csv")
# Remove all rows, where sla_data£status is Cancelled
# sla_data <- sla_data %>%
#   filter(stage != "Cancelled")
# Merge data and sla_data, where the number and closed of data is the same as task and stop time of sla_data
sla_data <- select(sla_data_clean, task = task, business_percentage = business_percentage, duration = duration, end_time = end_time)
#remove duplicates of sla_data$task
sla_data <- sla_data %>%
distinct(task, .keep_all = TRUE)
data <- merge(x = data, y = sla_data,
by.x = c("number", "closed"),
by.y = c("task", "end_time"),
all.x = TRUE)
# Save the data
write.csv(data, "Data/data.csv")
write_csv(data, "Data/data.csv")
#save dataframe
save(data, file = "Data/data.RData")
#save dataframe as rda
save(data, file = "Data/data.Rda")
#save dataframe as json
write_json(data, "Data/data.json")
View(data)
# Save the data
write.csv(data, "Data/data.csv",encoding="UTF-8")
# Save the data
write.csv(data, "Data/data.csv",encoding="utf-8")
# Save the data
write.csv(data, "Data/data.csv",row.names = FALSE)
View(data)
#read json file
cases_data <- read_csv("Data/u_cases_thesis.csv", encoding="UTF-8", stringsAsFactors=FALSE)
#read json file
cases_data <- read_csv("Data/u_cases_thesis.csv", encoding="UTF-8")
#read json file
cases_data <- read_csv("Data/u_cases_thesis.csv", encoding="utf-8")
#read json file
read_csv?
cases_data <- read_csv("Data/u_cases_thesis.csv", encoding="UTF-8")
#read json file
read_csv?
cases_data <- read_csv("Data/u_cases_thesis.csv", encoding="UTF-8")
#read json file
read_csv?
cases_data <- read_csv("Data/u_cases_thesis.csv", encoding="UTF-8")
#read json file
read_csv?
cases_data <- read_csv("Data/u_cases_thesis.csv", encoding="UTF-8")
#read json file
cases_data <- read_csv("Data/u_cases_thesis.csv", locale = locale(encoding = "UTF-8"))
View(cases_data)
View(cases_data)
data <- cases_data # We make a copy from the original dataset and work on the copy
View(cases_data)
#read json file
test <- read_json("Data/u_cases_thesis.json")
#read json file
json <- fromJSON("Data/u_cases_thesis.json")
View(data)
#read json file
cases_data <- read_csv("Data/Test.csv", locale = locale(encoding = "UTF-8"))
View(cases_data)
## Import data
# We set the working directory and we call the data that we are going to use. Please import the csv file.
rm(list=ls())
set.seed(7)
setwd("C:/Thesis")
#read json file
cases_data <- read_csv("Data/sn_customerservice_case_utf8.csv", locale = locale(encoding = "UTF-8"))
data <- cases_data # We make a copy from the original dataset and work on the copy
data <- select(cases_data, number = ca_number, case = ca_case, description = ca_description, case_type = ca_u_case_type, due_date = ca_due_date, first_response_time = ca_first_response_time, closed = ca_closed_at, opened = ca_opened_at, account = ca_account, contact = ca_contact, created_by = ca_sys_created_by, business_service = ca_business_service, business_service_activity = ca_u_business_service_activity, assigned_to = ca_assigned_to, assignment_group = ca_assignment_group, auto_close = ca_auto_close, time_worked = ca_time_worked, reassignment_count = ca_reassignment_count, impact = ca_impact, priority = ca_priority, urgency = ca_urgency, escalation = ca_escalation, sla_breached = sla_has_breached, comments = ca_comments, case_cause = ca_u_case_cause, cause = ca_cause, close_notes = ca_close_notes, resolution_code = ca_resolution_code, problem = ca_problem)
View(data)
data <- select(cases_data, number = number, case = case, description = description, case_type = u_case_type, due_date = due_date, first_response_time = first_response_time, closed = closed_at, opened = opened_at, account = account, contact = contact, created_by = sys_created_by, business_service = business_service, business_service_activity = u_business_service_activity, assigned_to = assigned_to, assignment_group = assignment_group, auto_close = auto_close, time_worked = time_worked, reassignment_count = reassignment_count, impact = impact, priority = priority, urgency = urgency, escalation = escalation, sla_breached = sla_has_breached, comments = comments, case_cause = u_case_cause, cause = cause, close_notes = close_notes, resolution_code = resolution_code, problem = problem)
View(cases_data)
data <- select(cases_data, number = number, case = case, description = description, case_type = u_case_type, due_date = due_date, first_response_time = first_response_time, closed = closed_at, opened = opened_at, account = account, contact = contact, created_by = sys_created_by, business_service = business_service, business_service_activity = u_business_service_activity, assigned_to = assigned_to, assignment_group = assignment_group, auto_close = auto_close, time_worked = time_worked, reassignment_count = reassignment_count, impact = impact, priority = priority, urgency = urgency, escalation = escalation, comments = comments, case_cause = u_case_cause, cause = cause, close_notes = close_notes, resolution_code = resolution_code, problem = problem)
# Transforming dates to Timestamp
data$opened <- strptime(data$opened, "%d-%m-%Y %H:%M:%S", tz="UTC")
data <- data %>%
filter(Year(opened) <= Year("2024-02-01")) %>%
filter(Year(opened) >= Year("2020-01-01"))
# merg data sets
sla_data_clean <- read_csv("Data/task_sla.csv")
View(sla_data_clean)
sla_data <- select(sla_data_clean, task = task, business_percentage = business_percentage, sla_has_breached, has_breached, duration = duration, end_time = end_time)
sla_data <- select(sla_data_clean, task = task, business_percentage = business_percentage, sla_has_breached = has_breached, duration = duration, end_time = end_time)
#remove duplicates of sla_data$task
sla_data <- sla_data %>%
distinct(task, .keep_all = TRUE)
data <- merge(x = data, y = sla_data,
by.x = c("number", "closed"),
by.y = c("task", "end_time"),
all.x = TRUE)
# Save the data
write.csv(data, "Data/data.csv",row.names = FALSE)
View(data)
# Transforming the type of the variables
data$opened <- as.POSIXct(data$opened, format = "%Y-%m-%d %H:%M:%S", tz = "UTC")
data$closed <- as.POSIXct(data$closed, format = "%Y-%m-%d %H:%M:%S", tz = "UTC")
# Transforming the type of the variables
data$opened <- as.POSIXct(data$opened, format = "%Y-%m-%d %H:%M:%S", tz = "UTC")
data$closed <- as.POSIXct(data$closed, format = "%Y-%m-%d %H:%M:%S", tz = "UTC")
data$due_date <- as.POSIXct(data$due_date, format = "%Y-%m-%d %H:%M:%S", tz = "UTC")
data$first_response_time <- as.POSIXct(data$first_response_time, format = "%Y-%m-%d %H:%M:%S", tz = "UTC")
data$auto_close <- as.POSIXct(data$auto_close, format = "%Y-%m-%d %H:%M:%S", tz = "UTC")
# Transforming the type of the variables
data$opened <- as.POSIXct(data$opened, format = "%Y-%m-%d %H:%M:%S", tz = "UTC")
data$closed <- as.POSIXct(data$closed, format = "%Y-%m-%d %H:%M:%S", tz = "UTC")
data$due_date <- as.POSIXct(data$due_date, format = "%Y-%m-%d %H:%M:%S", tz = "UTC")
data$first_response_time <- as.POSIXct(data$first_response_time, format = "%Y-%m-%d %H:%M:%S", tz = "UTC")
data$auto_close <- as.factor(data$auto_close)
data$end_time <- as.POSIXct(data$end_time, format = "%Y-%m-%d %H:%M:%S", tz = "UTC")
# Transforming the type of the variables
data$opened <- as.POSIXct(data$opened, format = "%Y-%m-%d %H:%M:%S", tz = "UTC")
data$closed <- as.POSIXct(data$closed, format = "%Y-%m-%d %H:%M:%S", tz = "UTC")
data$due_date <- as.POSIXct(data$due_date, format = "%Y-%m-%d %H:%M:%S", tz = "UTC")
data$first_response_time <- as.POSIXct(data$first_response_time, format = "%Y-%m-%d %H:%M:%S", tz = "UTC")
data$auto_close <- as.factor(data$auto_close)
data$business_percentage <- as.numeric(data$business_percentage)
data$sla_has_breached <- as.factor(data$sla_has_breached)
data$reassignment_count <- as.numeric(data$reassignment_count)
data$impact <- as.factor(data$impact)
data$priority <- as.factor(data$priority)
data$urgency <- as.factor(data$urgency)
data$escalation <- as.factor(data$escalation)
data$case_cause <- as.factor(data$case_cause)
data$resolution_code <- as.factor(data$resolution_code)
data$problem <- as.factor(data$problem)
data$assignment_group <- as.factor(data$assignment_group)
data$assigned_to <- as.factor(data$assigned_to)
data$created_by <- as.factor(data$created_by)
data$business_service <- as.factor(data$business_service)
data$business_service_activity <- as.factor(data$business_service_activity)
data$case_type <- as.factor(data$case_type)
data$account <- as.factor(data$account)
data$contact <- as.factor(data$contact)
# Save the data
write.csv(data, "Data/data.csv",row.names = FALSE)
View(data)
## Import data
# We set the working directory and we call the data that we are going to use. Please import the csv file.
rm(list=ls())
set.seed(7)
setwd("C:/Thesis")
#read json file
cases_data <- read_csv("Data/sn_customerservice_case_utf8.csv", locale = locale(encoding = "UTF-8"))
data <- cases_data # We make a copy from the original dataset and work on the copy
View(data)
data <- select(cases_data, number = number, case = case, description = description, case_type = u_case_type, due_date = due_date, first_response_time = first_response_time, closed = closed_at, opened = opened_at, account = account, contact = contact, created_by = sys_created_by, business_service = business_service, business_service_activity = u_business_service_activity, assigned_to = assigned_to, assignment_group = assignment_group, auto_close = auto_close, time_worked = time_worked, reassignment_count = reassignment_count, impact = impact, priority = priority, urgency = urgency, escalation = escalation, comments = comments, case_cause = u_case_cause, cause = cause, close_notes = close_notes, resolution_code = resolution_code, problem = problem)
View(data)
# Filter between dates and removing rows who are not closed
data <- data %>%
filter(Year(strptime(data$opened, "%d-%m-%Y %H:%M:%S", tz="UTC")) <= Year("2024-02-01")) %>%
filter(Year(strptime(data$opened, "%d-%m-%Y %H:%M:%S", tz="UTC")) >= Year("2020-01-01")) %>%
filter(!is.na(closed))
View(data)
# Filter between dates and removing rows who are not closed
data <- data %>%
filter(Year(strptime(data$opened, "%d-%m-%Y %H:%M:%S", tz="UTC")) <= Year("2024-02-01")) %>%
filter(Year(strptime(data$opened, "%d-%m-%Y %H:%M:%S", tz="UTC")) >= Year("2020-01-01")) %>%
filter(!is.na(closed))
# merg data sets
sla_data_clean <- read_csv("Data/task_sla.csv")
# Remove all rows, where sla_data£status is Cancelled
# sla_data <- sla_data %>%
#   filter(stage != "Cancelled")
# Merge data and sla_data, where the number and closed of data is the same as task and stop time of sla_data
sla_data <- select(sla_data_clean, task = task, business_percentage = business_percentage, sla_has_breached = has_breached, duration = duration, end_time = end_time)
#remove duplicates of sla_data$task
sla_data <- sla_data %>%
distinct(task, .keep_all = TRUE)
data <- merge(x = data, y = sla_data,
by.x = c("number", "closed"),
by.y = c("task", "end_time"),
all.x = TRUE)
View(data)
# Transforming the type of the variables
data$opened <- as.POSIXct(data$opened, format = "%Y-%m-%d %H:%M:%S", tz = "UTC")
data$closed <- as.POSIXct(data$closed, format = "%Y-%m-%d %H:%M:%S", tz = "UTC")
data$due_date <- as.POSIXct(data$due_date, format = "%Y-%m-%d %H:%M:%S", tz = "UTC")
data$first_response_time <- as.POSIXct(data$first_response_time, format = "%Y-%m-%d %H:%M:%S", tz = "UTC")
data$auto_close <- as.factor(data$auto_close)
data$business_percentage <- as.numeric(data$business_percentage)
data$sla_has_breached <- as.factor(data$sla_has_breached)
data$reassignment_count <- as.numeric(data$reassignment_count)
data$impact <- as.factor(data$impact)
data$priority <- as.factor(data$priority)
data$urgency <- as.factor(data$urgency)
data$escalation <- as.factor(data$escalation)
data$case_cause <- as.factor(data$case_cause)
data$resolution_code <- as.factor(data$resolution_code)
data$problem <- as.factor(data$problem)
data$assignment_group <- as.factor(data$assignment_group)
data$assigned_to <- as.factor(data$assigned_to)
data$created_by <- as.factor(data$created_by)
data$business_service <- as.factor(data$business_service)
data$business_service_activity <- as.factor(data$business_service_activity)
data$case_type <- as.factor(data$case_type)
data$account <- as.factor(data$account)
data$contact <- as.factor(data$contact)
View(data)
View(data)
knitr::opts_chunk$set(echo = TRUE)
# We start by installing the necessary libraries. Make sure to uncomment and run
# only the libraries which you haven't installed already.
#install.packages("DescTools")
#install.packages("xgboost")
#install.packages("caret")
#install.packages("dplyr")
#install.packages("tidyverse")
#install.packages("mltools")
#install.packages("reshape2")
#install.packages("data.table")
#install.packages("pracma")
#install.packages("rsample")
# install.packages("PRROC")
# install.packages("e1071")
# install.packages("dlookr")
# install.packages("pROC")
# install.packages("ROCR")
# install.packages("nnet")
library(DescTools)
library(xgboost)
library(caret)
library(dplyr)
library(tidyverse)
library(mltools)
library(reshape2)
library(data.table)
library(pracma)
library(rsample)
library(PRROC)
library(e1071)
library(dlookr)
library(pROC)
library(ROCR)
library(nnet)
library(ggplot2)
options(scipen=999)
## Import data
# We set the working directory and we call the data that we are going to use. Please import the csv file.
rm(list=ls())
set.seed(7)
setwd("C:/Thesis")
#read json file
cases_data <- read_csv("Data/sn_customerservice_case_utf8.csv", locale = locale(encoding = "UTF-8"))
data <- cases_data # We make a copy from the original dataset and work on the copy
data <- select(cases_data, number = number, case = case, description = description, case_type = u_case_type, due_date = due_date, first_response_time = first_response_time, closed = closed_at, opened = opened_at, account = account, contact = contact, created_by = sys_created_by, business_service = business_service, business_service_activity = u_business_service_activity, assigned_to = assigned_to, assignment_group = assignment_group, auto_close = auto_close, time_worked = time_worked, reassignment_count = reassignment_count, impact = impact, priority = priority, urgency = urgency, escalation = escalation, comments = comments, case_cause = u_case_cause, cause = cause, close_notes = close_notes, resolution_code = resolution_code, problem = problem)
# Filter between dates and removing rows who are not closed
data <- data %>%
filter(Year(strptime(data$opened, "%d-%m-%Y %H:%M:%S", tz="UTC")) <= Year("2024-02-01")) %>%
filter(Year(strptime(data$opened, "%d-%m-%Y %H:%M:%S", tz="UTC")) >= Year("2020-01-01")) %>%
filter(!is.na(closed))
# merg data sets
sla_data_clean <- read_csv("Data/task_sla.csv")
# Remove all rows, where sla_data£status is Cancelled
# sla_data <- sla_data %>%
#   filter(stage != "Cancelled")
# Merge data and sla_data, where the number and closed of data is the same as task and stop time of sla_data
sla_data <- select(sla_data_clean, task = task, business_percentage = business_percentage, sla_has_breached = has_breached, duration = duration, end_time = end_time)
#remove duplicates of sla_data$task
sla_data <- sla_data %>%
distinct(task, .keep_all = TRUE)
data <- merge(x = data, y = sla_data,
by.x = c("number", "closed"),
by.y = c("task", "end_time"),
all.x = TRUE)
# Transforming the type of the variables
data$opened <- as.POSIXct(data$opened, strptime(data$opened, "%d-%m-%Y %H:%M:%S", tz="UTC"))
# Transforming the type of the variables
data$opened <- as.POSIXct(strptime(data$opened, "%d-%m-%Y %H:%M:%S", tz="UTC"))
data$closed <- as.POSIXct(strptime(data$closed, "%d-%m-%Y %H:%M:%S", tz="UTC"))
data$due_date <- as.POSIXct(strptime(data$due_date, "%d-%m-%Y %H:%M:%S", tz="UTC"))
data$first_response_time <- as.POSIXct(strptime(data$first_response_time, "%d-%m-%Y %H:%M:%S", tz="UTC"))
data$auto_close <- as.factor(data$auto_close)
data$business_percentage <- as.numeric(data$business_percentage)
data$sla_has_breached <- as.factor(data$sla_has_breached)
data$reassignment_count <- as.numeric(data$reassignment_count)
data$impact <- as.factor(data$impact)
data$priority <- as.factor(data$priority)
data$urgency <- as.factor(data$urgency)
data$escalation <- as.factor(data$escalation)
data$case_cause <- as.factor(data$case_cause)
data$resolution_code <- as.factor(data$resolution_code)
data$problem <- as.factor(data$problem)
data$assignment_group <- as.factor(data$assignment_group)
data$assigned_to <- as.factor(data$assigned_to)
data$created_by <- as.factor(data$created_by)
data$business_service <- as.factor(data$business_service)
data$business_service_activity <- as.factor(data$business_service_activity)
data$case_type <- as.factor(data$case_type)
data$account <- as.factor(data$account)
data$contact <- as.factor(data$contact)
View(data)
# Save the data
write.csv(data, "Data/data.csv",row.names = FALSE)
# Let's run basic check on the dataset
summary(data)
str(data)
table(data$Attrition)
#PercTable(data$ca_time_worked)
View(data)
