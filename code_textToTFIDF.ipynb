{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import re\n",
    "from wordcloud import WordCloud\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from ydata_profiling import ProfileReport\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "# your code that triggers the warning goes here\n",
    "\n",
    "pd.options.mode.chained_assignment = 'warn'  # set it back to the default value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "# Specifying data types for columns while reading a CSV file\n",
    "dtype_dict = {\n",
    "    \"number\": \"str\",\n",
    "    \"closed\": \"str\", \n",
    "    \"case\": \"str\",\n",
    "    \"description\": \"str\",\n",
    "    \"case_type\": \"category\",\n",
    "    \"due_date\": \"str\", \n",
    "    \"first_response_time\": \"str\", \n",
    "    \"opened\": \"str\",  \n",
    "    \"account\": \"category\",\n",
    "    \"contact\": \"category\",\n",
    "    \"created_by\": \"category\",\n",
    "    \"business_service\": \"category\",\n",
    "    \"business_service_activity\": \"category\",\n",
    "    \"assigned_to\": \"category\",\n",
    "    \"assignment_group\": \"category\",\n",
    "    \"auto_close\": \"category\",\n",
    "    \"time_worked\": \"float\",\n",
    "    \"reassignment_count\": \"int\",\n",
    "    \"impact\": \"category\",\n",
    "    \"priority\": \"category\",\n",
    "    \"urgency\": \"category\",\n",
    "    \"escalation\": \"category\",\n",
    "    \"comments\": \"str\",\n",
    "    \"case_cause\": \"category\",\n",
    "    \"cause\": \"str\",\n",
    "    \"close_notes\": \"str\",\n",
    "    \"resolution_code\": \"category\",\n",
    "    \"problem\": \"category\",\n",
    "    \"business_percentage\": \"float\",\n",
    "    \"sla_has_breached\": \"category\",\n",
    "    \"duration\": \"float\",\n",
    "    \"openedToClosed\": \"float\",\n",
    "    \"created_by_group\": \"category\"\n",
    "}\n",
    "\n",
    "# read csv file into dataframe\n",
    "df = pd.read_csv('Data/data.csv', dtype = dtype_dict)\n",
    "\n",
    "# concat short_description and description columns. Handle NaN values\n",
    "df['short_description'] = df['short_description'].fillna('')\n",
    "df['description'] = df['description'].fillna('')\n",
    "df['description'] = df['short_description'].str.cat(df['description'], sep =\" \")\n",
    "\n",
    "# print shape of dataframe \n",
    "print(df.shape)\n",
    "print(df.columns)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# profile = ProfileReport(df, title=\"Profiling Report\")\n",
    "# profile.to_notebook_iframe()\n",
    "# profile.to_file(f\"./html/Profiling Report Overall.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# displau Short Description (case), Description, Cause and close_notes\n",
    "df_text = df[['number', 'description', 'cause', 'close_notes']]\n",
    "\n",
    "# preprocess df_text columns \n",
    "\n",
    "display(df_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_captions(data, column):\n",
    "    data[column] = data[column].apply(lambda x: x.replace('\\n', ' '))\n",
    "    data[column] = data[column].apply(lambda x: x.replace('-', ' '))\n",
    "    data[column] = data[column].apply(lambda x: x.lower())\n",
    "    data[column] = data[column].apply(lambda x: re.sub('[^a-zA-ZäÄöÖüÜ\\s]', ' ', x))\n",
    "    return data[column]\n",
    "\n",
    "# change datatype of columns to string\n",
    "df_text['description'] = df_text['description'].astype(str).copy()\n",
    "df_text['cause'] = df_text['cause'].astype(str).copy()\n",
    "df_text['close_notes'] = df_text['close_notes'].astype(str).copy()\n",
    "\n",
    "df_text['description'] = process_captions(df_text, 'description').copy()\n",
    "df_text['cause'] = process_captions(df_text, 'cause').copy()\n",
    "df_text['close_notes'] = process_captions(df_text, 'close_notes').copy()\n",
    "\n",
    "display(df_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "stopwords = requests.get(\"https://raw.githubusercontent.com/stopwords-iso/stopwords-de/master/stopwords-de.txt\").text.split(\"\\n\")\n",
    "# Add common words to stopwords\n",
    "stopwords.extend([\"nan\", \"frau\", \"herr\", \"name\", \"ch\", \"bitte\", \"und\", \"die\", \"das\", \"ist\", \"zu\", \"den\", \"der\", \"es\", \"ein\", \"sie\", \n",
    "                        \"nicht\", \"von\", \"mit\", \"dem\", \"sich\", \"auf\", \"für\", \"an\", \"sind\", \"des\", \"wird\", \"dass\", \"im\", \"auch\", \"als\", \n",
    "                        \"an\", \"nach\", \"wie\", \"aber\", \"aus\", \"bei\", \"durch\", \"hat\", \"man\", \"noch\", \"einem\", \"über\", \"einer\", \"um\", \"am\", \n",
    "                        \"ohne\", \"zwischen\", \"so\", \"nur\", \"zum\", \"kann\", \"vor\", \"dieser\", \"bis\", \"habe\", \"wenn\", \"sein\", \"wird\", \"wurde\", \n",
    "                        \"können\", \"gegen\", \"dann\", \"müssen\", \"diese\", \"weil\", \"welche\", \"oder\", \"zwei\", \"eines\", \"mehr\", \"Jahre\", \"wieder\", \n",
    "                        \"keine\", \"hallo\", \"grüsse\", \"gruss\", \"vielen\", \"besten\", \"dank\", \"guten\", \"morgen\", \"tag\", \"freundlich\", \"liebe\", \n",
    "                        \"lieber\", \"sehr geehrte\", \"geehrter\", \"geehrte\", \"hi\", \"de\", \"guten tag\", \"beste\", \"herzliche\", \"liebe grüße\", \"vielen dank\", \n",
    "                        \"besten dank\", \"freundliche\", \"grüße\", \"danke\", \"siehe\", \"tel\", \"mail\", \"mehr\", \"à\", \"vous\", \"la\", \"le\", \"e\", \"en\", \"et\", \"mon\", \"je\", \n",
    "                        \"les\", \"pas\", \"que\", \"a\", \"c\", \"e\", \"compt\", \"est\", \"une\", \"il\", \"the\", \"at\"])\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Remove all characters after \"Freundliche Grüsse\" or \"Beste Grüsse\"\n",
    "df_text['description'] = df_text['description'].apply(lambda x: x.split('freundliche gr')[0])\n",
    "df_text['description'] = df_text['description'].apply(lambda x: x.split('beste gr')[0])\n",
    "df_text['description'] = df_text['description'].apply(lambda x: x.split('vielen dank')[0])\n",
    "df_text['description'] = df_text['description'].apply(lambda x: x.split('von:')[0])\n",
    "\n",
    "\n",
    "# Create empty df to store the top 50 words for each column\n",
    "top_50_words = pd.DataFrame()\n",
    "\n",
    "for column in df_text.columns[1:]:\n",
    "    tfidf = TfidfVectorizer(stop_words=stopwords)\n",
    "    text = tfidf.fit_transform(df_text[column])\n",
    "\n",
    "    VectorizedText = pd.DataFrame(text.toarray(), columns=tfidf.get_feature_names_out())\n",
    "    # Add to all columns the column name\n",
    "    VectorizedText.columns = [column + \"_\" + col for col in VectorizedText.columns]\n",
    "    column_sums = VectorizedText.sum(axis=0).sort_values(ascending=False)\n",
    "\n",
    "    # Select the names of the top 50 columns with the highest sums\n",
    "    top_50_columns = column_sums.head(15).index\n",
    "    top_50_words[column] = top_50_columns\n",
    "\n",
    "    # Subset the original DataFrame to keep only these top 50 columns\n",
    "    minimized_df = VectorizedText[top_50_columns]\n",
    "\n",
    "    # Add from the original df the column number to the minimized_df at first position\n",
    "    minimized_df.insert(0, 'number', df_text['number'])\n",
    "\n",
    "    # save minimized_df to csv\n",
    "    minimized_df.to_csv('Data/VectorizedText_' + column + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For topic Modelling\n",
    "top_50_words = pd.DataFrame()\n",
    "for column in df_text.columns[1:]:\n",
    "    tfidf = TfidfVectorizer(stop_words=stopwords)\n",
    "    text = tfidf.fit_transform(df_text[column])\n",
    "\n",
    "    VectorizedText = pd.DataFrame(text.toarray(), columns=tfidf.get_feature_names_out())\n",
    "    # Add to all columns the column name\n",
    "    # VectorizedText.columns = [column + \"_\" + col for col in VectorizedText.columns]\n",
    "    column_sums = VectorizedText.sum(axis=0).sort_values(ascending=False)\n",
    "\n",
    "    # Select the names of the top 50 columns with the highest sums\n",
    "    top_50_columns = column_sums.head(150).index\n",
    "    top_50_words[column] = top_50_columns\n",
    "\n",
    "    # only remain words in df_text that are in top_50_words\n",
    "    df_text2 = df_text[['number', column]].copy()\n",
    "    df_text2[column] = df_text2[column].apply(lambda x: ' '.join([word for word in x.split() if word in top_50_words['description'].values]))\n",
    "\n",
    "\n",
    "    # save minimized_df to csv\n",
    "    df_text2.to_csv('Data/TopicModel_VectorizedText_' + column + '.csv', index=False)\n",
    "\n",
    "    # count number of empty values\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
